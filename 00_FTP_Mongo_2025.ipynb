{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expresso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coleção 'expresso_2025' já existe.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndocumento_exemplo = {\\n    \"DATA_CRIACAO\": \"2025-04-24\",\\n    \"CENTRO\": \"Lisboa\",\\n    \"Giro\": \"G123\",\\n    \"LOPTICA\": \"LO1\",\\n    \"JANELA_HORARIA\": \"08:00-10:00\",\\n    \"NOME\": \"João Silva\",\\n    \"MORADA\": \"Rua das Flores, 123\",\\n    \"CP\": \"1000-000\",\\n    \"LOCALIDADE\": \"Lisboa\",\\n    \"COD_T_EVEN\": \"EVT456\",\\n    \"DATA_EVENTO\": \"2025-04-23\",\\n    \"LATITUDE\": \"38.7169\",\\n    \"LONGITUDE\": \"-9.1399\",\\n    \"NOME_REM\": \"Maria Silva\",\\n    \"COD_PAIS_ORIGEM\": \"PT\"\\n}\\n\\n#inserir_documento(\"expresso_2025\", documento_exemplo)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "from pymongo.errors import PyMongoError\n",
    "\n",
    "\n",
    "def conecta_db():\n",
    "    client = MongoClient('mongodb://localhost:27017/')\n",
    "    db = client['ctt2025']\n",
    "    return db\n",
    "\n",
    "\n",
    "def criar_colecao(nome_colecao):\n",
    "    db = conecta_db()\n",
    "    if nome_colecao not in db.list_collection_names():\n",
    "        db.create_collection(nome_colecao)\n",
    "        print(f\"Coleção '{nome_colecao}' criada.\")\n",
    "    else:\n",
    "        print(f\"Coleção '{nome_colecao}' já existe.\")\n",
    "\n",
    "\n",
    "def inserir_documento(nome_colecao, documento):\n",
    "    db = conecta_db()\n",
    "    colecao = db[nome_colecao]\n",
    "    try:\n",
    "        colecao.insert_one(documento)\n",
    "        print(\"Documento inserido com sucesso.\")\n",
    "    except PyMongoError as error:\n",
    "        print(\"Erro ao inserir documento:\", error)\n",
    "\n",
    "\n",
    "# Exemplo de uso\n",
    "criar_colecao(\"expresso_2025\")\n",
    "\n",
    "# Documento de exemplo (substitui pelos dados reais)\n",
    "'''\n",
    "documento_exemplo = {\n",
    "    \"DATA_CRIACAO\": \"2025-04-24\",\n",
    "    \"CENTRO\": \"Lisboa\",\n",
    "    \"Giro\": \"G123\",\n",
    "    \"LOPTICA\": \"LO1\",\n",
    "    \"JANELA_HORARIA\": \"08:00-10:00\",\n",
    "    \"NOME\": \"João Silva\",\n",
    "    \"MORADA\": \"Rua das Flores, 123\",\n",
    "    \"CP\": \"1000-000\",\n",
    "    \"LOCALIDADE\": \"Lisboa\",\n",
    "    \"COD_T_EVEN\": \"EVT456\",\n",
    "    \"DATA_EVENTO\": \"2025-04-23\",\n",
    "    \"LATITUDE\": \"38.7169\",\n",
    "    \"LONGITUDE\": \"-9.1399\",\n",
    "    \"NOME_REM\": \"Maria Silva\",\n",
    "    \"COD_PAIS_ORIGEM\": \"PT\"\n",
    "}\n",
    "\n",
    "#inserir_documento(\"expresso_2025\", documento_exemplo)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20250823' '20250824' '20250825' '20250826' '20250827' '20250828'\n",
      " '20250829' '20250830' '20250831' '20250901' '20250902' '20250903'\n",
      " '20250904' '20250905' '20250906' '20250907' '20250908' '20250909'\n",
      " '20250910' '20250911' '20250912' '20250913' '20250914' '20250915'\n",
      " '20250916' '20250917' '20250918' '20250919' '20250920' '20250921'\n",
      " '20250922' '20250923' '20250924' '20250925' '20250926' '20250927'\n",
      " '20250928' '20250929' '20250930' '20251001' '20251002']\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime  # <<< ADIÇÃO: para detetar datetime/date\n",
    "\n",
    "def conecta_db():\n",
    "    client = MongoClient('mongodb://localhost:27017/')\n",
    "    return client['ctt2025']\n",
    "\n",
    "# Conectar à base de dados\n",
    "db = conecta_db()\n",
    "colecao = db['expresso_2025']\n",
    "\n",
    "# Obter todas as datas distintas do campo DATA_CRIACAO\n",
    "datas = colecao.distinct(\"data_criacao\")\n",
    "\n",
    "data_validation = []\n",
    "\n",
    "# Expressão regular para datas no formato DD/MM/YYYY (legado)\n",
    "#regex_data = re.compile(r\"^(\\d{4})\\.(\\d{1,2})\\.(\\d{1,2})$\")\n",
    "#regex_data = re.compile(r\"^(\\d{4})-(\\d{1,2})-(\\d{1,2})$\")\n",
    "regex_data = re.compile(r\"^(\\d{2})/(\\d{2})/(\\d{4})$\")\n",
    "\n",
    "for data in datas:\n",
    "    if isinstance(data, (datetime.datetime, datetime.date)):  # <<< ADIÇÃO\n",
    "        y = data.year\n",
    "        m = f\"{data.month:02d}\"\n",
    "        d = f\"{data.day:02d}\"\n",
    "        data_validation.append(f\"{y}{m}{d}\")\n",
    "    elif isinstance(data, str):\n",
    "        match = regex_data.match(data.strip())\n",
    "        if match:\n",
    "            dia, mes, ano = match.groups()\n",
    "            data_formatada = ano + mes.zfill(2) + dia.zfill(2)\n",
    "            data_validation.append(data_formatada)\n",
    "        else:\n",
    "            print(f\"Formato inválido ignorado: {data}\")\n",
    "    else:\n",
    "        # tipos inesperados (None, etc.)\n",
    "        print(f\"Tipo de data não suportado ignorado: {type(data)} -> {data}\")\n",
    "\n",
    "# Eliminar duplicados, ordenar e converter para numpy array\n",
    "data_validation = np.array(sorted(set(data_validation)))\n",
    "print(data_validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ficheiro 'EVPDA3466_ENV_20250906_20250907045626.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250907_20250908010515.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250908_20250909010504.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250909_20250910010505.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250910_20250911010508.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250911_20250912010502.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250912_20250913010520.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250913_20250914010502.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250914_20250915010510.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250915_20250916010528.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250916_20250917010521.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250917_20250918010502.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250918_20250919010516.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250919_20250920010504.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250920_20250921010509.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250921_20250922010515.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250922_20250923010525.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250923_20250924010509.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250924_20250925010517.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250925_20250926010501.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250926_20250927010511.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250927_20250928010523.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250928_20250929010503.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250929_20250930010519.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250930_20251001010520.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20251001_20251002010521.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20251002_20251003010503.CSV' ignorado (já inserido ou data inválida).\n",
      "A processar: 20251003\n",
      "Ficheiro 'EVPDA3466_ENV_20251003_20251004010504.CSV' inserido com sucesso (518985 registos). Tempo: 762.58s\n",
      "A processar: 20251004\n"
     ]
    }
   ],
   "source": [
    "from ftplib import FTP\n",
    "import os\n",
    "import io\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Conexão MongoDB\n",
    "def conecta_db():\n",
    "    client = MongoClient('mongodb://localhost:27017/')\n",
    "    return client['ctt2025']\n",
    "\n",
    "# Verifica datas já existentes na coleção REDE_BASE_2025\n",
    "def obter_datas_existentes():\n",
    "    db = conecta_db()\n",
    "    colecao = db[\"expresso_2025\"]\n",
    "    datas = colecao.distinct(\"data_criacao\")\n",
    "\n",
    "    regex_data = re.compile(r\"^(\\d{2})/(\\d{2})/(\\d{4})$\")\n",
    "\n",
    "    datas_formatadas = []\n",
    "\n",
    "    for data in datas:\n",
    "        # >>> ALTERAÇÃO (suporta datetime e legado string DD/MM/YYYY)\n",
    "        if isinstance(data, (datetime.date, datetime.datetime)):\n",
    "            y = data.year\n",
    "            m = f\"{data.month:02d}\"\n",
    "            d = f\"{data.day:02d}\"\n",
    "            datas_formatadas.append(f\"{y}{m}{d}\")\n",
    "        elif isinstance(data, str):\n",
    "            match = regex_data.match(data.strip())\n",
    "            if match:\n",
    "                dia, mes, ano = match.groups()\n",
    "                data_formatada = ano + mes.zfill(2) + dia.zfill(2)\n",
    "                datas_formatadas.append(data_formatada)\n",
    "        # <<<\n",
    "\n",
    "    return set(datas_formatadas)\n",
    "\n",
    "# Processa linha\n",
    "def process_data(row):\n",
    "    try:\n",
    "        decoded_line = row.decode('Latin-1')\n",
    "    except UnicodeDecodeError:\n",
    "        decoded_line = row.decode('utf-8')\n",
    "\n",
    "    if len(decoded_line.split(';')) == 13:\n",
    "        return decoded_line.strip()\n",
    "    else:\n",
    "        print(f\"Ignorando linha com colunas incorretas: {decoded_line}\")\n",
    "        return None\n",
    "\n",
    "# Ligar FTP e obter ficheiros\n",
    "ftp = FTP()\n",
    "ftp.connect(host='10.0.25.193', port=2122)\n",
    "ftp.login(user='cax_sgc', passwd='gy768#lo')\n",
    "ftp.cwd('/OP/Rotas/EventosPDA')\n",
    "files = ftp.nlst()\n",
    "ftp.quit()\n",
    "\n",
    "# Obter datas validadas\n",
    "data_validation = obter_datas_existentes()\n",
    "\n",
    "# Processar ficheiros\n",
    "for file in files:\n",
    "    start_time = time.time()\n",
    "    if file.startswith(\"E\"):\n",
    "        date_str_s = file[14:22].replace(\"/\", \"\")\n",
    "        ano = file[14:18]\n",
    "        mes = file[18:20]\n",
    "\n",
    "        if date_str_s not in data_validation and ano == \"2025\" and mes >= \"01\":\n",
    "            print(f\"A processar: {date_str_s}\")\n",
    "\n",
    "            ftp = FTP()\n",
    "            ftp.connect(host='10.0.25.193', port=2122)\n",
    "            ftp.login(user='cax_sgc', passwd='gy768#lo')\n",
    "            ftp.cwd('/OP/Rotas/EventosPDA')\n",
    "\n",
    "            file_data = io.BytesIO()\n",
    "            ftp.retrbinary('RETR ' + file, file_data.write)\n",
    "            file_data.seek(0)\n",
    "\n",
    "            try:\n",
    "                df = pd.read_csv(file_data, sep=';', on_bad_lines='skip', encoding=\"Latin-1\")\n",
    "            except pd.errors.ParserError as pe:\n",
    "                print(f\"Erro ao ler CSV no ficheiro '{file}': {pe}\")\n",
    "                ftp.quit()\n",
    "                continue\n",
    "\n",
    "            ftp.quit()\n",
    "\n",
    "            if df.empty:\n",
    "                print(f\"Ficheiro '{file}' vazio ou com linhas inválidas. Ignorado.\")\n",
    "                continue\n",
    "\n",
    "            # >>> ALTERAÇÃO: nomes de colunas em minúsculas\n",
    "            df.columns = [c.lower() for c in df.columns]\n",
    "            # <<<\n",
    "\n",
    "            # Mantém limpeza de nulos como estava\n",
    "            df = df.where(pd.notna(df), None)\n",
    "\n",
    "            # >>> ALTERAÇÃO: data_criacao como datetime (Date no Mongo)\n",
    "            dia_i = int(date_str_s[6:8])\n",
    "            mes_i = int(mes)\n",
    "            ano_i = int(ano)\n",
    "            data_dt = datetime.datetime(ano_i, mes_i, dia_i)\n",
    "            df[\"data_criacao\"] = data_dt\n",
    "            # <<<\n",
    "\n",
    "            # >>> ALTERAÇÃO: restantes campos como string\n",
    "            for col in df.columns:\n",
    "                if col != \"data_criacao\":\n",
    "                    df[col] = df[col].astype(str)\n",
    "                    df[col] = df[col].replace([\"nan\", \"None\"], \"\")\n",
    "            # <<<\n",
    "\n",
    "            db = conecta_db()\n",
    "            colecao = db['expresso_2025']\n",
    "\n",
    "            try:\n",
    "                documentos = df.to_dict(orient='records')\n",
    "                if documentos:\n",
    "                    colecao.insert_many(documentos)\n",
    "                    print(f\"Ficheiro '{file}' inserido com sucesso ({len(documentos)} registos). Tempo: {time.time() - start_time:.2f}s\")\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao inserir dados do ficheiro '{file}': {e}\")\n",
    "        else:\n",
    "            print(f\"Ficheiro '{file}' ignorado (já inserido ou data inválida).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Banca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coleção 'banca_2025' já existe.\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "from pymongo.errors import PyMongoError\n",
    "\n",
    "# Conexão MongoDB\n",
    "def conecta_db():\n",
    "    client = MongoClient('mongodb://localhost:27017/')\n",
    "    db = client['ctt2025']\n",
    "    return db\n",
    "\n",
    "# Criar coleção (opcional em MongoDB)\n",
    "def criar_colecao(nome_colecao):\n",
    "    db = conecta_db()\n",
    "    if nome_colecao not in db.list_collection_names():\n",
    "        db.create_collection(nome_colecao)\n",
    "        print(f\"Coleção '{nome_colecao}' criada.\")\n",
    "    else:\n",
    "        print(f\"Coleção '{nome_colecao}' já existe.\")\n",
    "\n",
    "# Inserir um documento (linha)\n",
    "def inserir_documento(nome_colecao, documento):\n",
    "    db = conecta_db()\n",
    "    colecao = db[nome_colecao]\n",
    "    try:\n",
    "        colecao.insert_one(documento)\n",
    "        print(\"Documento inserido com sucesso.\")\n",
    "    except PyMongoError as error:\n",
    "        print(f\"Erro ao inserir documento: {error}\")\n",
    "\n",
    "# Criar a coleção (equivalente a \"CREATE TABLE IF NOT EXISTS\")\n",
    "criar_colecao(\"banca_2025\")\n",
    "\n",
    "# Exemplo de inserção (podes adaptar à tua fonte de dados)\n",
    "documento_exemplo = {\n",
    "    \"COD_CLIENTE_BNC\": \"123456\",\n",
    "    \"CODIGO_BALCAO_SIDIR\": \"001\",\n",
    "    \"CODIGO_BALCAO_AF\": \"AF456\",\n",
    "    \"AGENCIA\": \"Lisboa\",\n",
    "    \"CA\": \"Central\",\n",
    "    \"CIRCUITO\": \"C1\",\n",
    "    \"DATA\": \"24/04/2025\",\n",
    "    \"CONTACTO\": \"João Silva\",\n",
    "    \"FATURAR\": \"Sim\",\n",
    "    \"ENTREGUE\": \"Não\",\n",
    "    \"TAREFA\": \"Entrega\",\n",
    "    \"CP\": \"1000-001\",\n",
    "    \"CENTRO_OPERACIONAL\": \"Centro A\",\n",
    "    \"MORADA_TOQUE\": \"Rua das Flores\",\n",
    "    \"HORA_TOQUE_PREVISTA\": \"10:00\",\n",
    "    \"HORA_TOQUE_REAL\": \"10:15\"\n",
    "}\n",
    "\n",
    "# Inserir esse documento na coleção\n",
    "#inserir_documento(\"banca_2025\", documento_exemplo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def conecta_db():\n",
    "    client = MongoClient('mongodb://localhost:27017/')\n",
    "    return client['ctt2025']\n",
    "\n",
    "# Conectar à base de dados\n",
    "db = conecta_db()\n",
    "colecao = db['banca_2025']\n",
    "\n",
    "# Obter todas as datas distintas do campo DATA\n",
    "datas = colecao.distinct(\"data\")\n",
    "\n",
    "data_validation = []\n",
    "\n",
    "# Expressão regular correta: YYYY.MM.DD\n",
    "regex_data = re.compile(r\"^(\\d{4})\\.(\\d{1,2})\\.(\\d{1,2})$\")\n",
    "\n",
    "for data in datas:\n",
    "    if isinstance(data, str):\n",
    "        match = regex_data.match(data.strip())\n",
    "        if match:\n",
    "            ano, mes, dia = match.groups()\n",
    "            data_formatada = ano + mes.zfill(2) + dia.zfill(2)\n",
    "            data_validation.append(data_formatada)\n",
    "        else:\n",
    "            print(f\"Formato inválido ignorado: {data}\")\n",
    "\n",
    "# Eliminar duplicados, ordenar e converter para numpy array\n",
    "data_validation = np.array(sorted(set(data_validation)))\n",
    "print(data_validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IGNORADO: CAX019_TOQUE_BANCA_D_20250903_20250904000626.CSV (data: 20250903) — já inserido ou fora do ano alvo.\n",
      "IGNORADO: CAX019_TOQUE_BANCA_D_20250904_20250905000614.CSV (data: 20250904) — já inserido ou fora do ano alvo.\n",
      "IGNORADO: CAX019_TOQUE_BANCA_D_20250905_20250906000607.CSV (data: 20250905) — já inserido ou fora do ano alvo.\n",
      "IGNORADO: CAX019_TOQUE_BANCA_D_20250908_20250909000621.CSV (data: 20250908) — já inserido ou fora do ano alvo.\n",
      "IGNORADO: CAX019_TOQUE_BANCA_D_20250909_20250910000617.CSV (data: 20250909) — já inserido ou fora do ano alvo.\n",
      "IGNORADO: CAX019_TOQUE_BANCA_D_20250910_20250911000623.CSV (data: 20250910) — já inserido ou fora do ano alvo.\n",
      "IGNORADO: CAX019_TOQUE_BANCA_D_20250911_20250912000607.CSV (data: 20250911) — já inserido ou fora do ano alvo.\n",
      "IGNORADO: CAX019_TOQUE_BANCA_D_20250912_20250913000613.CSV (data: 20250912) — já inserido ou fora do ano alvo.\n",
      "IGNORADO: CAX019_TOQUE_BANCA_D_20250915_20250916000609.CSV (data: 20250915) — já inserido ou fora do ano alvo.\n",
      "IGNORADO: CAX019_TOQUE_BANCA_D_20250916_20250917000608.CSV (data: 20250916) — já inserido ou fora do ano alvo.\n",
      "IGNORADO: CAX019_TOQUE_BANCA_D_20250917_20250918000626.CSV (data: 20250917) — já inserido ou fora do ano alvo.\n",
      "IGNORADO: CAX019_TOQUE_BANCA_D_20250918_20250919000615.CSV (data: 20250918) — já inserido ou fora do ano alvo.\n",
      "IGNORADO: CAX019_TOQUE_BANCA_D_20250919_20250920000607.CSV (data: 20250919) — já inserido ou fora do ano alvo.\n",
      "IGNORADO: CAX019_TOQUE_BANCA_D_20250922_20250923000645.CSV (data: 20250922) — já inserido ou fora do ano alvo.\n",
      "IGNORADO: CAX019_TOQUE_BANCA_D_20250923_20250924000611.CSV (data: 20250923) — já inserido ou fora do ano alvo.\n",
      "IGNORADO: CAX019_TOQUE_BANCA_D_20250924_20250925000614.CSV (data: 20250924) — já inserido ou fora do ano alvo.\n",
      "IGNORADO: CAX019_TOQUE_BANCA_D_20250925_20250926000613.CSV (data: 20250925) — já inserido ou fora do ano alvo.\n",
      "IGNORADO: CAX019_TOQUE_BANCA_D_20250926_20250927000604.CSV (data: 20250926) — já inserido ou fora do ano alvo.\n",
      "IGNORADO: CAX019_TOQUE_BANCA_D_20250929_20250930000630.CSV (data: 20250929) — já inserido ou fora do ano alvo.\n",
      "IGNORADO: CAX019_TOQUE_BANCA_D_20250930_20251001000605.CSV (data: 20250930) — já inserido ou fora do ano alvo.\n",
      "IGNORADO: CAX019_TOQUE_BANCA_D_20251001_20251002000616.CSV (data: 20251001) — já inserido ou fora do ano alvo.\n",
      "IGNORADO: CAX019_TOQUE_BANCA_D_20251002_20251003000610.CSV (data: 20251002) — já inserido ou fora do ano alvo.\n"
     ]
    }
   ],
   "source": [
    "# Conexão FTP\n",
    "ftp = FTP()\n",
    "ftp.connect(host='10.0.25.193', port=2122)\n",
    "ftp.login(user='cax_sgc', passwd='gy768#lo')\n",
    "ftp.cwd('/OP/Rotas/Banca_Recolhas')\n",
    "files = ftp.nlst()\n",
    "ftp.quit()\n",
    "\n",
    "# Datas já presentes\n",
    "data_validation = obter_datas_existentes()\n",
    "\n",
    "# Processamento\n",
    "for file in files:\n",
    "    start_time = time.time()\n",
    "    if file.startswith(\"C\"):\n",
    "        matches = re.findall(r\"\\d{8}\", file)\n",
    "        if matches:\n",
    "            date_str_s = matches[0]  # usa a primeira data no nome\n",
    "            ano = date_str_s[:4]\n",
    "        else:\n",
    "            print(f\"⚠️ Nome inválido sem data no ficheiro: {file}\")\n",
    "            continue\n",
    "\n",
    "        #print(f\"→ A verificar ficheiro: {file}\")\n",
    "        #print(f\"  Data extraída: {date_str_s}\")\n",
    "        #print(f\"  Está em data_validation? {'SIM' if date_str_s in data_validation else 'NÃO'}\")\n",
    "\n",
    "        if date_str_s not in data_validation and ano == \"2025\":\n",
    "            # Reabrir conexão FTP para o ficheiro\n",
    "            ftp = FTP()\n",
    "            ftp.connect(host='10.0.25.193', port=2122)\n",
    "            ftp.login(user='cax_sgc', passwd='gy768#lo')\n",
    "            ftp.cwd('/OP/Rotas/Banca_Recolhas')\n",
    "\n",
    "            file_data = io.BytesIO()\n",
    "            ftp.retrbinary('RETR ' + file, file_data.write)\n",
    "            file_data.seek(0)\n",
    "            lines = file_data.readlines()\n",
    "            ftp.quit()\n",
    "\n",
    "            def process_data(row):\n",
    "                try:\n",
    "                    return row.decode('Latin-1').strip()\n",
    "                except UnicodeDecodeError:\n",
    "                    return row.decode('utf-8').strip()\n",
    "\n",
    "            data_rows = [process_data(row) for row in lines[:-1]]\n",
    "\n",
    "            if data_rows and data_rows[0].startswith('\\ufeff'):\n",
    "                data_rows[0] = data_rows[0][1:]\n",
    "\n",
    "            csv_data = '\\n'.join(data_rows[:-1]).encode('utf-8')\n",
    "            decoded_data = csv_data.decode('utf-8-sig')\n",
    "\n",
    "            if decoded_data.strip():\n",
    "                df = pd.read_csv(io.StringIO(decoded_data), sep=';', on_bad_lines='skip', encoding=\"Latin-1\")\n",
    "\n",
    "                column_mapping = {\n",
    "                    'ï»¿COD_CLIENTE_BNC': 'COD_CLIENTE_BNC',\n",
    "                    'CODIGO_BALCAO_SIDIR': 'CODIGO_BALCAO_SIDIR',\n",
    "                    'CODIGO_BALCAO_AF': 'CODIGO_BALCAO_AF',\n",
    "                    'AGENCIA': 'AGENCIA',\n",
    "                    'CA': 'CA',\n",
    "                    'CIRCUITO': 'CIRCUITO',\n",
    "                    'DATA': 'DATA',\n",
    "                    'CONTACTO': 'CONTACTO',\n",
    "                    'FATURAR': 'FATURAR',\n",
    "                    'ENTREGUE': 'ENTREGUE',\n",
    "                    'TAREFA': 'TAREFA',\n",
    "                    'CP': 'CP',\n",
    "                    'CENTRO OPERACIONAL': 'CENTRO_OPERACIONAL',\n",
    "                    'MORADA TOQUE': 'MORADA_TOQUE',\n",
    "                    'HORA TOQUE PREVISTA': 'HORA_TOQUE_PREVISTA',\n",
    "                    'HORA TOQUE REAL': 'HORA_TOQUE_REAL'\n",
    "                }\n",
    "\n",
    "                df.rename(columns=column_mapping, inplace=True)\n",
    "                df = df.where(pd.notna(df), None)\n",
    "\n",
    "                # Forçar DATA no formato YYYY.MM.DD\n",
    "                data_formatada = f\"{date_str_s[:4]}.{date_str_s[4:6]}.{date_str_s[6:]}\"\n",
    "                df['data'] = data_formatada\n",
    "                print(f\"→ Será inserida a data: {data_formatada}\")\n",
    "\n",
    "                db = conecta_db()\n",
    "                colecao = db['banca_2025']\n",
    "\n",
    "                try:\n",
    "                    documentos = df.to_dict(orient='records')\n",
    "                    if documentos:\n",
    "                        colecao.insert_many(documentos)\n",
    "                        print(f\"✅ Ficheiro '{file}' inserido com sucesso ({len(documentos)} registos). Tempo: {time.time() - start_time:.2f}s\")\n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Erro ao inserir dados do ficheiro '{file}': {e}\")\n",
    "        else:\n",
    "            print(f\"IGNORADO: {file} (data: {date_str_s}) — já inserido ou fora do ano alvo.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "recolhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coleção 'recolhas_2025' já existe.\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "from pymongo.errors import PyMongoError\n",
    "\n",
    "# Conexão MongoDB\n",
    "def conecta_db():\n",
    "    client = MongoClient('mongodb://localhost:27017/')\n",
    "    db = client['ctt2025']\n",
    "    return db\n",
    "\n",
    "# Criar coleção (opcional — Mongo cria automaticamente ao inserir)\n",
    "def criar_colecao(nome_colecao):\n",
    "    db = conecta_db()\n",
    "    if nome_colecao not in db.list_collection_names():\n",
    "        db.create_collection(nome_colecao)\n",
    "        print(f\"Coleção '{nome_colecao}' criada.\")\n",
    "    else:\n",
    "        print(f\"Coleção '{nome_colecao}' já existe.\")\n",
    "\n",
    "# Inserir documento (linha)\n",
    "def inserir_documento(nome_colecao, documento):\n",
    "    db = conecta_db()\n",
    "    colecao = db[nome_colecao]\n",
    "    try:\n",
    "        colecao.insert_one(documento)\n",
    "        print(\"Documento inserido com sucesso.\")\n",
    "    except PyMongoError as error:\n",
    "        print(f\"Erro ao inserir documento: {error}\")\n",
    "\n",
    "# Criar coleção equivalente à tabela Recolhas_2025\n",
    "criar_colecao(\"recolhas_2025\")\n",
    "\n",
    "# Exemplo de documento (linha)\n",
    "documento_exemplo = {\n",
    "    \"data_ficheiro\": \"2025.04.24\",\n",
    "    \"Código Orgânico\": \"ORG001\",\n",
    "    \"Descrição\": \"Serviço diário\",\n",
    "    \"Dia\": \"Quinta-feira\",\n",
    "    \"Giro\": \"G1\",\n",
    "    \"Tipo\": \"Normal\",\n",
    "    \"Ponto Recolha\": \"Ponto A\",\n",
    "    \"Nome Cliente\": \"Cliente XPTO\",\n",
    "    \"Morada de recolha\": \"Rua das Flores, 10\",\n",
    "    \"Código postal recolha\": \"1000-001\",\n",
    "    \"Qntd de Objetos Recolhidos\": \"12\",\n",
    "    \"Estado\": \"Concluído\",\n",
    "    \"Hora Início\": \"08:00\",\n",
    "    \"Hora Fim\": \"10:00\",\n",
    "    \"Hora Real\": \"09:35\"\n",
    "}\n",
    "\n",
    "# Inserir esse documento na coleção\n",
    "#inserir_documento(\"Recolhas_2025\", documento_exemplo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20250823' '20250824' '20250825' '20250826' '20250827' '20250828'\n",
      " '20250829' '20250830' '20250831' '20250901' '20250902' '20250903'\n",
      " '20250904' '20250905' '20250906' '20250907' '20250908' '20250909'\n",
      " '20250910' '20250911' '20250912' '20250913' '20250914' '20250915'\n",
      " '20250916' '20250917' '20250918' '20250919' '20250920' '20250921']\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def conecta_db():\n",
    "    client = MongoClient('mongodb://localhost:27017/')\n",
    "    return client['ctt2025']\n",
    "\n",
    "# Conectar à base de dados\n",
    "db = conecta_db()\n",
    "colecao = db['recolhas_2025']\n",
    "\n",
    "# Obter todas as datas distintas do campo data_ficheiro\n",
    "datas = colecao.distinct(\"data_ficheiro\")\n",
    "\n",
    "data_validation = []\n",
    "\n",
    "# Expressão regular para datas no formato YYYY.MM.DD\n",
    "regex_data = re.compile(r\"^(\\d{4})\\.(\\d{2})\\.(\\d{2})$\")\n",
    "\n",
    "\n",
    "\n",
    "for data in datas:\n",
    "    if isinstance(data, str):\n",
    "        match = regex_data.match(data.strip())\n",
    "        if match:\n",
    "            ano, mes, dia = match.groups()\n",
    "            data_formatada = ano + mes.zfill(2) + dia.zfill(2)\n",
    "            data_validation.append(data_formatada)\n",
    "        else:\n",
    "            print(f\"Formato inválido ignorado: {data}\")\n",
    "\n",
    "# Eliminar duplicados, ordenar e converter para numpy array\n",
    "data_validation = np.array(sorted(set(data_validation)))\n",
    "print(data_validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ficheiro 'RECSG4366_RECOLHAS_20250903_20250904010500.CSV' ignorado (já existente ou fora do ano alvo).\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250904_20250905010501.CSV' ignorado (já existente ou fora do ano alvo).\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250905_20250906010500.CSV' ignorado (já existente ou fora do ano alvo).\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250906_20250907045637.CSV' ignorado (já existente ou fora do ano alvo).\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250907_20250908010500.CSV' ignorado (já existente ou fora do ano alvo).\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250908_20250909010500.CSV' ignorado (já existente ou fora do ano alvo).\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250909_20250910010500.CSV' ignorado (já existente ou fora do ano alvo).\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250910_20250911010501.CSV' ignorado (já existente ou fora do ano alvo).\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250911_20250912010500.CSV' ignorado (já existente ou fora do ano alvo).\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250912_20250913010501.CSV' ignorado (já existente ou fora do ano alvo).\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250913_20250914010500.CSV' ignorado (já existente ou fora do ano alvo).\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250914_20250915010500.CSV' ignorado (já existente ou fora do ano alvo).\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250915_20250916010500.CSV' ignorado (já existente ou fora do ano alvo).\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250916_20250917010500.CSV' ignorado (já existente ou fora do ano alvo).\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250917_20250918010500.CSV' ignorado (já existente ou fora do ano alvo).\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250918_20250919010500.CSV' ignorado (já existente ou fora do ano alvo).\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250919_20250920010500.CSV' ignorado (já existente ou fora do ano alvo).\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250920_20250921010500.CSV' ignorado (já existente ou fora do ano alvo).\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250921_20250922010500.CSV' ignorado (já existente ou fora do ano alvo).\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250922_20250923010500.CSV' inserido com sucesso (15556 registos). Tempo: 27.74s\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250923_20250924010501.CSV' inserido com sucesso (14458 registos). Tempo: 28.01s\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250924_20250925010500.CSV' inserido com sucesso (13814 registos). Tempo: 5.82s\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250925_20250926010501.CSV' inserido com sucesso (13734 registos). Tempo: 12.80s\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250926_20250927010501.CSV' inserido com sucesso (12823 registos). Tempo: 2.04s\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250927_20250928010501.CSV' inserido com sucesso (4052 registos). Tempo: 2.00s\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250928_20250929010501.CSV' inserido com sucesso (4102 registos). Tempo: 6.37s\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250929_20250930010501.CSV' inserido com sucesso (14768 registos). Tempo: 17.94s\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250930_20251001010501.CSV' inserido com sucesso (14814 registos). Tempo: 14.14s\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20251001_20251002010500.CSV' inserido com sucesso (14118 registos). Tempo: 12.91s\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20251002_20251003010500.CSV' inserido com sucesso (14060 registos). Tempo: 11.65s\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import time\n",
    "from ftplib import FTP\n",
    "from pymongo import MongoClient\n",
    "import re\n",
    "\n",
    "# Conexão MongoDB\n",
    "def conecta_db():\n",
    "    client = MongoClient('mongodb://localhost:27017/')\n",
    "    return client['ctt2025']\n",
    "\n",
    "# Obter datas já existentes em MongoDB (Recolhas_2025)\n",
    "def obter_datas_existentes():\n",
    "    db = conecta_db()\n",
    "    colecao = db[\"recolhas_2025\"]\n",
    "    datas = colecao.distinct(\"data_ficheiro\")\n",
    "\n",
    "    regex_data = re.compile(r\"^(\\d{4})\\.(\\d{2})\\.(\\d{2})$\")\n",
    "\n",
    "\n",
    "    datas_formatadas = []\n",
    "\n",
    "    for data in datas:\n",
    "        if isinstance(data, str):\n",
    "            match = regex_data.match(data.strip())\n",
    "            if match:\n",
    "                ano, mes, dia = match.groups()\n",
    "                data_formatada = ano + mes + dia\n",
    "                datas_formatadas.append(data_formatada)\n",
    "            else:\n",
    "                print(f\"Formato inválido ignorado: {data}\")\n",
    "\n",
    "    return set(datas_formatadas)\n",
    "\n",
    "\n",
    "# Processamento de cada linha do ficheiro\n",
    "def process_data(row):\n",
    "    try:\n",
    "        return row.decode('Latin-1').strip()\n",
    "    except UnicodeDecodeError:\n",
    "        return row.decode('utf-8').strip()\n",
    "\n",
    "# Nomes das colunas\n",
    "columns = ['\"Código Orgânico\"', '\"Descrição\"', '\"Dia\"', '\"Giro\"', '\"Tipo\"', '\"Ponto Recolha\"', '\"Nome Cliente\"',\n",
    "           '\"Morada de recolha\"', '\"Código postal recolha\"', '\"Qntd de Objetos Recolhidos\"', '\"Estado\"', '\"Hora Início\"',\n",
    "           '\"Hora Fim\"', '\"Hora Real\"']\n",
    "\n",
    "# Obter lista de ficheiros do FTP\n",
    "ftp = FTP()\n",
    "ftp.connect(host='10.0.25.193', port=2122)\n",
    "ftp.login(user='cax_sgc', passwd='gy768#lo')\n",
    "ftp.cwd('/OP/Rotas/Banca_Recolhas')\n",
    "files = ftp.nlst()\n",
    "ftp.quit()\n",
    "\n",
    "# Datas já presentes no MongoDB\n",
    "data_validation = obter_datas_existentes()\n",
    "\n",
    "# Processar ficheiros\n",
    "for file in files:\n",
    "    start_time = time.time()\n",
    "    if file.startswith(\"R\"):\n",
    "        date_str_s = file[19:27].replace(\"/\", \"\")\n",
    "        ano = file[19:23]\n",
    "\n",
    "        if date_str_s not in data_validation and ano == \"2025\":\n",
    "            # Nova ligação FTP para o ficheiro\n",
    "            ftp = FTP()\n",
    "            ftp.connect(host='10.0.25.193', port=2122)\n",
    "            ftp.login(user='cax_sgc', passwd='gy768#lo')\n",
    "            ftp.cwd('/OP/Rotas/Banca_Recolhas')\n",
    "\n",
    "            file_data = io.BytesIO()\n",
    "            ftp.retrbinary('RETR ' + file, file_data.write)\n",
    "            file_data.seek(0)\n",
    "            lines = file_data.readlines()\n",
    "            data_rows = [process_data(row) for row in lines[:-1]]\n",
    "            ftp.quit()\n",
    "\n",
    "            if data_rows and data_rows[0].startswith('\\ufeff'):\n",
    "                data_rows[0] = data_rows[0][1:]\n",
    "\n",
    "            csv_data = '\\n'.join(data_rows[:-1]).encode('utf-8')\n",
    "            decoded_data = csv_data.decode('utf-8-sig')\n",
    "\n",
    "            if decoded_data.strip():\n",
    "                df = pd.read_csv(io.StringIO(decoded_data), sep=';', names=columns, on_bad_lines='skip', encoding=\"Latin-1\")\n",
    "                df = df.iloc[1:]  # remover header duplicado\n",
    "                df['data_ficheiro'] = f\"{ano}.{date_str_s[4:6]}.{date_str_s[6:]}\"\n",
    "                df = df.where(pd.notna(df), None)\n",
    "\n",
    "                db = conecta_db()\n",
    "                colecao = db['recolhas_2025']\n",
    "\n",
    "                try:\n",
    "                    documentos = df.to_dict(orient='records')\n",
    "                    if documentos:\n",
    "                        colecao.insert_many(documentos)\n",
    "                        print(f\"Ficheiro '{file}' inserido com sucesso ({len(documentos)} registos). Tempo: {time.time() - start_time:.2f}s\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Erro ao inserir dados do ficheiro '{file}': {e}\")\n",
    "        else:\n",
    "            print(f\"Ficheiro '{file}' ignorado (já existente ou fora do ano alvo).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rede base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coleção 'rede_base_2025' já existe.\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "from pymongo.errors import PyMongoError\n",
    "\n",
    "# Conexão MongoDB\n",
    "def conecta_db():\n",
    "    client = MongoClient('mongodb://localhost:27017/')\n",
    "    return client['ctt2025']\n",
    "\n",
    "# Criar coleção (opcional — Mongo cria automaticamente ao inserir)\n",
    "def criar_colecao(nome_colecao):\n",
    "    db = conecta_db()\n",
    "    if nome_colecao not in db.list_collection_names():\n",
    "        db.create_collection(nome_colecao)\n",
    "        print(f\"Coleção '{nome_colecao}' criada.\")\n",
    "    else:\n",
    "        print(f\"Coleção '{nome_colecao}' já existe.\")\n",
    "\n",
    "# Inserir documento na coleção\n",
    "def inserir_documento(nome_colecao, documento):\n",
    "    db = conecta_db()\n",
    "    colecao = db[nome_colecao]\n",
    "\n",
    "    # Verificação para evitar duplicados com base na DATA_CRIACAO\n",
    "    data_chave = documento.get(\"data_criacao\")\n",
    "    if data_chave:\n",
    "        existe = colecao.find_one({\"data_criacao\": data_chave})\n",
    "        if existe:\n",
    "            print(f\"Documento com DATA_CRIACAO '{data_chave}' já existe. Ignorado.\")\n",
    "            return\n",
    "\n",
    "    try:\n",
    "        colecao.insert_one(documento)\n",
    "        print(\"Documento inserido com sucesso.\")\n",
    "    except PyMongoError as error:\n",
    "        print(f\"Erro ao inserir documento: {error}\")\n",
    "\n",
    "# Criar coleção\n",
    "criar_colecao(\"rede_base_2025\")\n",
    "\n",
    "# Exemplo de documento\n",
    "documento_exemplo = {\n",
    "    \"DATA_CRIACAO\": \"2025.04.24\",\n",
    "    \"CENTRO\": \"Centro Norte\",\n",
    "    \"Giro\": \"G45\",\n",
    "    \"LOPTICA\": \"L1234\",\n",
    "    \"JANELA_HORARIA\": \"08:00-10:00\",\n",
    "    \"NOME\": \"Loja XPTO\",\n",
    "    \"MORADA\": \"Rua Exemplo, 100\",\n",
    "    \"CP\": \"4000-123\",\n",
    "    \"LOCALIDADE\": \"Porto\",\n",
    "    \"COD_T_EVEN\": \"EV123\",\n",
    "    \"DATA_EVENTO\": \"2025.04.23\",\n",
    "    \"LATITUDE\": \"41.14961\",\n",
    "    \"LONGITUDE\": \"-8.61099\",\n",
    "    \"NOME_REM\": \"Remetente ABC\",\n",
    "    \"COD_PAIS_ORIGEM\": \"PT\"\n",
    "}\n",
    "\n",
    "# Inserir documento\n",
    "#inserir_documento(\"REDE_BASE_2025\", documento_exemplo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20250823' '20250824' '20250825' '20250826' '20250827' '20250828'\n",
      " '20250829' '20250830' '20250831' '20250901' '20250902' '20250903'\n",
      " '20250904' '20250905' '20250906' '20250907' '20250908' '20250909'\n",
      " '20250910' '20250911' '20250912' '20250913' '20250914' '20250915'\n",
      " '20250916' '20250917' '20250918' '20250919' '20250920' '20250921']\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def conecta_db():\n",
    "    client = MongoClient('mongodb://localhost:27017/')\n",
    "    return client['ctt2025']\n",
    "\n",
    "# Conectar à base de dados\n",
    "db = conecta_db()\n",
    "colecao = db['rede_base_2025']\n",
    "\n",
    "# Obter todas as datas distintas do campo DATA_CRIACAO\n",
    "datas = colecao.distinct(\"data_criacao\")\n",
    "\n",
    "data_validation = []\n",
    "\n",
    "# Expressão regular para datas no formato YYYY.MM.DD\n",
    "regex_data = re.compile(r\"^(\\d{2})/(\\d{2})/(\\d{4})$\")\n",
    "\n",
    "for data in datas:\n",
    "    if isinstance(data, str):\n",
    "        match = regex_data.match(data.strip())\n",
    "        if match:\n",
    "            dia, mes, ano = match.groups()\n",
    "            data_formatada = ano + mes.zfill(2) + dia.zfill(2)\n",
    "            data_validation.append(data_formatada)\n",
    "        else:\n",
    "            print(f\"Formato inválido ignorado: {data}\")\n",
    "\n",
    "# Eliminar duplicados, ordenar e converter para numpy array\n",
    "data_validation = np.array(sorted(set(data_validation)))\n",
    "print(data_validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ficheiro 'EVRBP4348_ENV_20250903_20250904010512.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVRBP4348_ENV_20250904_20250905010506.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVRBP4348_ENV_20250905_20250906010507.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVRBP4348_ENV_20250906_20250907045626.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVRBP4348_ENV_20250907_20250908010515.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVRBP4348_ENV_20250908_20250909010504.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVRBP4348_ENV_20250909_20250910010505.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVRBP4348_ENV_20250910_20250911010508.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVRBP4348_ENV_20250911_20250912010502.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVRBP4348_ENV_20250912_20250913010520.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVRBP4348_ENV_20250913_20250914010502.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVRBP4348_ENV_20250914_20250915010510.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVRBP4348_ENV_20250915_20250916010528.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVRBP4348_ENV_20250916_20250917010521.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVRBP4348_ENV_20250917_20250918010502.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVRBP4348_ENV_20250918_20250919010516.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVRBP4348_ENV_20250919_20250920010504.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVRBP4348_ENV_20250920_20250921010509.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVRBP4348_ENV_20250921_20250922010515.CSV' ignorado (já inserido ou data inválida).\n",
      "A processar: 20250922\n",
      "Ficheiro 'EVRBP4348_ENV_20250922_20250923010525.CSV' inserido com sucesso (306286 registos). Tempo: 246.55s\n",
      "A processar: 20250923\n",
      "Ficheiro 'EVRBP4348_ENV_20250923_20250924010509.CSV' inserido com sucesso (272852 registos). Tempo: 212.91s\n",
      "A processar: 20250924\n",
      "Ficheiro 'EVRBP4348_ENV_20250924_20250925010517.CSV' inserido com sucesso (250907 registos). Tempo: 254.83s\n",
      "A processar: 20250925\n",
      "Ficheiro 'EVRBP4348_ENV_20250925_20250926010501.CSV' inserido com sucesso (237142 registos). Tempo: 443.97s\n",
      "A processar: 20250926\n",
      "Ficheiro 'EVRBP4348_ENV_20250926_20250927010511.CSV' inserido com sucesso (245150 registos). Tempo: 301.10s\n",
      "A processar: 20250927\n",
      "Ficheiro 'EVRBP4348_ENV_20250927_20250928010523.CSV' inserido com sucesso (6915 registos). Tempo: 12.20s\n",
      "A processar: 20250928\n",
      "Ficheiro 'EVRBP4348_ENV_20250928_20250929010503.CSV' inserido com sucesso (1 registos). Tempo: 0.64s\n",
      "A processar: 20250929\n",
      "Ficheiro 'EVRBP4348_ENV_20250929_20250930010519.CSV' inserido com sucesso (266743 registos). Tempo: 106.39s\n",
      "A processar: 20250930\n",
      "Ficheiro 'EVRBP4348_ENV_20250930_20251001010520.CSV' inserido com sucesso (305935 registos). Tempo: 110.88s\n",
      "A processar: 20251001\n",
      "Ficheiro 'EVRBP4348_ENV_20251001_20251002010521.CSV' inserido com sucesso (291131 registos). Tempo: 328.62s\n",
      "A processar: 20251002\n",
      "Ficheiro 'EVRBP4348_ENV_20251002_20251003010503.CSV' inserido com sucesso (265958 registos). Tempo: 275.39s\n"
     ]
    }
   ],
   "source": [
    "from ftplib import FTP\n",
    "import os\n",
    "import io\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Conexão MongoDB\n",
    "def conecta_db():\n",
    "    client = MongoClient('mongodb://localhost:27017/')\n",
    "    return client['ctt2025']\n",
    "\n",
    "# Verifica datas já existentes na coleção REDE_BASE_2025\n",
    "def obter_datas_existentes():\n",
    "    db = conecta_db()\n",
    "    colecao = db[\"rede_base_2025\"]\n",
    "    datas = colecao.distinct(\"data_criacao\")\n",
    "\n",
    "    regex_data = re.compile(r\"^(\\d{2})/(\\d{2})/(\\d{4})$\")\n",
    "    datas_formatadas = []\n",
    "\n",
    "    for data in datas:\n",
    "        if isinstance(data, str):\n",
    "            match = regex_data.match(data.strip())\n",
    "            if match:\n",
    "                dia, mes, ano = match.groups()\n",
    "                data_formatada = ano + mes.zfill(2) + dia.zfill(2)\n",
    "                datas_formatadas.append(data_formatada)\n",
    "    \n",
    "    return set(datas_formatadas)\n",
    "\n",
    "# Processa linha\n",
    "def process_data(row):\n",
    "    try:\n",
    "        decoded_line = row.decode('Latin-1')\n",
    "    except UnicodeDecodeError:\n",
    "        decoded_line = row.decode('utf-8')\n",
    "\n",
    "    if len(decoded_line.split(';')) == 13:\n",
    "        return decoded_line.strip()\n",
    "    else:\n",
    "        print(f\"Ignorando linha com colunas incorretas: {decoded_line}\")\n",
    "        return None\n",
    "\n",
    "# Ligar FTP e obter ficheiros\n",
    "ftp = FTP()\n",
    "ftp.connect(host='10.0.25.193', port=2122)\n",
    "ftp.login(user='cax_sgc', passwd='gy768#lo')\n",
    "ftp.cwd('/OP/Rotas/EventosPDA_Rede_base')\n",
    "files = ftp.nlst()\n",
    "ftp.quit()\n",
    "\n",
    "# Obter datas validadas\n",
    "data_validation = obter_datas_existentes()\n",
    "\n",
    "# Processar ficheiros\n",
    "for file in files:\n",
    "    start_time = time.time()\n",
    "    if file.startswith(\"E\"):\n",
    "        date_str_s = file[14:22].replace(\"/\", \"\")\n",
    "        ano = file[14:18]\n",
    "        mes = file[18:20]\n",
    "\n",
    "        if date_str_s not in data_validation and ano == \"2025\" and mes >= \"01\":\n",
    "            print(f\"A processar: {date_str_s}\")\n",
    "\n",
    "            ftp = FTP()\n",
    "            ftp.connect(host='10.0.25.193', port=2122)\n",
    "            ftp.login(user='cax_sgc', passwd='gy768#lo')\n",
    "            ftp.cwd('/OP/Rotas/EventosPDA_Rede_base')\n",
    "\n",
    "            file_data = io.BytesIO()\n",
    "            ftp.retrbinary('RETR ' + file, file_data.write)\n",
    "            file_data.seek(0)\n",
    "\n",
    "            try:\n",
    "                df = pd.read_csv(file_data, sep=';', on_bad_lines='skip', encoding=\"Latin-1\")\n",
    "            except pd.errors.ParserError as pe:\n",
    "                print(f\"Erro ao ler CSV no ficheiro '{file}': {pe}\")\n",
    "                ftp.quit()\n",
    "                continue\n",
    "\n",
    "            ftp.quit()\n",
    "\n",
    "            if df.empty:\n",
    "                print(f\"Ficheiro '{file}' vazio ou com linhas inválidas. Ignorado.\")\n",
    "                continue\n",
    "\n",
    "            df = df.where(pd.notna(df), None)\n",
    "            df[\"data_criacao\"] = f\"{date_str_s[6:]}/{mes}/{ano}\"\n",
    "\n",
    "\n",
    "            db = conecta_db()\n",
    "            colecao = db['rede_base_2025']\n",
    "\n",
    "            try:\n",
    "                documentos = df.to_dict(orient='records')\n",
    "                if documentos:\n",
    "                    colecao.insert_many(documentos)\n",
    "                    print(f\"Ficheiro '{file}' inserido com sucesso ({len(documentos)} registos). Tempo: {time.time() - start_time:.2f}s\")\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao inserir dados do ficheiro '{file}': {e}\")\n",
    "        else:\n",
    "            print(f\"Ficheiro '{file}' ignorado (já inserido ou data inválida).\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
