{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expresso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coleção 'expresso_2025' já existe.\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "from pymongo.errors import PyMongoError\n",
    "\n",
    "\n",
    "def conecta_db():\n",
    "    client = MongoClient('mongodb://localhost:27017/')\n",
    "    db = client['ctt2025']\n",
    "    return db\n",
    "\n",
    "\n",
    "def criar_colecao(nome_colecao):\n",
    "    db = conecta_db()\n",
    "    if nome_colecao not in db.list_collection_names():\n",
    "        db.create_collection(nome_colecao)\n",
    "        print(f\"Coleção '{nome_colecao}' criada.\")\n",
    "    else:\n",
    "        print(f\"Coleção '{nome_colecao}' já existe.\")\n",
    "\n",
    "\n",
    "def inserir_documento(nome_colecao, documento):\n",
    "    db = conecta_db()\n",
    "    colecao = db[nome_colecao]\n",
    "    try:\n",
    "        colecao.insert_one(documento)\n",
    "        print(\"Documento inserido com sucesso.\")\n",
    "    except PyMongoError as error:\n",
    "        print(\"Erro ao inserir documento:\", error)\n",
    "\n",
    "\n",
    "# Exemplo de uso\n",
    "criar_colecao(\"expresso_2025\")\n",
    "\n",
    "# Documento de exemplo (substitui pelos dados reais)\n",
    "documento_exemplo = {\n",
    "    \"DATA_CRIACAO\": \"2025-04-24\",\n",
    "    \"CENTRO\": \"Lisboa\",\n",
    "    \"Giro\": \"G123\",\n",
    "    \"LOPTICA\": \"LO1\",\n",
    "    \"JANELA_HORARIA\": \"08:00-10:00\",\n",
    "    \"NOME\": \"João Silva\",\n",
    "    \"MORADA\": \"Rua das Flores, 123\",\n",
    "    \"CP\": \"1000-000\",\n",
    "    \"LOCALIDADE\": \"Lisboa\",\n",
    "    \"COD_T_EVEN\": \"EVT456\",\n",
    "    \"DATA_EVENTO\": \"2025-04-23\",\n",
    "    \"LATITUDE\": \"38.7169\",\n",
    "    \"LONGITUDE\": \"-9.1399\",\n",
    "    \"NOME_REM\": \"Maria Silva\",\n",
    "    \"COD_PAIS_ORIGEM\": \"PT\"\n",
    "}\n",
    "\n",
    "#inserir_documento(\"expresso_2025\", documento_exemplo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipo de data não suportado ignorado: <class 'NoneType'> -> None\n",
      "['20250209' '20250210' '20250211' '20250212' '20250213' '20250214'\n",
      " '20250215' '20250216' '20250217' '20250218' '20250219' '20250220'\n",
      " '20250221' '20250222' '20250223' '20250224' '20250225' '20250226'\n",
      " '20250227' '20250228' '20250301' '20250302' '20250303' '20250304'\n",
      " '20250305' '20250306' '20250307' '20250308' '20250309' '20250310'\n",
      " '20250311' '20250312' '20250313' '20250314' '20250315' '20250316'\n",
      " '20250317' '20250318' '20250319' '20250320' '20250321' '20250322'\n",
      " '20250323' '20250324' '20250325' '20250326' '20250327' '20250328'\n",
      " '20250329' '20250330' '20250331' '20250401' '20250402' '20250403'\n",
      " '20250404' '20250405' '20250406' '20250407' '20250408' '20250409'\n",
      " '20250410' '20250411' '20250412' '20250413' '20250414' '20250415'\n",
      " '20250416' '20250417' '20250418' '20250420' '20250421' '20250422'\n",
      " '20250423' '20250425' '20250426' '20250427' '20250428' '20250429'\n",
      " '20250430' '20250501' '20250502' '20250503' '20250504' '20250505'\n",
      " '20250506' '20250507' '20250508' '20250509' '20250510' '20250511'\n",
      " '20250512' '20250513' '20250514' '20250515' '20250516' '20250517'\n",
      " '20250518' '20250519' '20250520' '20250521' '20250522' '20250523'\n",
      " '20250524' '20250525' '20250526' '20250527' '20250528' '20250529'\n",
      " '20250530' '20250531' '20250601' '20250602' '20250603' '20250604'\n",
      " '20250605' '20250606' '20250607' '20250608' '20250609' '20250610'\n",
      " '20250611' '20250612' '20250613' '20250614' '20250615' '20250616'\n",
      " '20250617' '20250618' '20250619' '20250620' '20250621' '20250622'\n",
      " '20250623' '20250624' '20250625' '20250626' '20250627' '20250628'\n",
      " '20250629' '20250630' '20250701' '20250702' '20250703' '20250704'\n",
      " '20250705' '20250706' '20250707' '20250708' '20250709' '20250710'\n",
      " '20250711' '20250712' '20250713' '20250714' '20250715' '20250716'\n",
      " '20250717' '20250718' '20250719' '20250720' '20250721' '20250722'\n",
      " '20250723' '20250724' '20250725' '20250726' '20250727' '20250728'\n",
      " '20250729' '20250730' '20250731' '20250801' '20250802' '20250803'\n",
      " '20250804' '20250805' '20250806' '20250807' '20250808' '20250809'\n",
      " '20250810' '20250811' '20250812' '20250813' '20250814' '20250815'\n",
      " '20250816' '20250817' '20250818' '20250819' '20250820' '20250821'\n",
      " '20250822' '20250823']\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime  # <<< ADIÇÃO: para detetar datetime/date\n",
    "\n",
    "def conecta_db():\n",
    "    client = MongoClient('mongodb://localhost:27017/')\n",
    "    return client['ctt2025']\n",
    "\n",
    "# Conectar à base de dados\n",
    "db = conecta_db()\n",
    "colecao = db['expresso_2025']\n",
    "\n",
    "# Obter todas as datas distintas do campo DATA_CRIACAO\n",
    "datas = colecao.distinct(\"data_criacao\")\n",
    "\n",
    "data_validation = []\n",
    "\n",
    "# Expressão regular para datas no formato DD/MM/YYYY (legado)\n",
    "#regex_data = re.compile(r\"^(\\d{4})\\.(\\d{1,2})\\.(\\d{1,2})$\")\n",
    "#regex_data = re.compile(r\"^(\\d{4})-(\\d{1,2})-(\\d{1,2})$\")\n",
    "regex_data = re.compile(r\"^(\\d{2})/(\\d{2})/(\\d{4})$\")\n",
    "\n",
    "for data in datas:\n",
    "    if isinstance(data, (datetime.datetime, datetime.date)):  # <<< ADIÇÃO\n",
    "        y = data.year\n",
    "        m = f\"{data.month:02d}\"\n",
    "        d = f\"{data.day:02d}\"\n",
    "        data_validation.append(f\"{y}{m}{d}\")\n",
    "    elif isinstance(data, str):\n",
    "        match = regex_data.match(data.strip())\n",
    "        if match:\n",
    "            dia, mes, ano = match.groups()\n",
    "            data_formatada = ano + mes.zfill(2) + dia.zfill(2)\n",
    "            data_validation.append(data_formatada)\n",
    "        else:\n",
    "            print(f\"Formato inválido ignorado: {data}\")\n",
    "    else:\n",
    "        # tipos inesperados (None, etc.)\n",
    "        print(f\"Tipo de data não suportado ignorado: {type(data)} -> {data}\")\n",
    "\n",
    "# Eliminar duplicados, ordenar e converter para numpy array\n",
    "data_validation = np.array(sorted(set(data_validation)))\n",
    "print(data_validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ficheiro 'EVPDA3466_ENV_20250727_20250728010524.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250728_20250729010500.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250729_20250730010517.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250730_20250731010503.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250731_20250801010504.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250801_20250802010514.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250802_20250803010501.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250803_20250804010506.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250804_20250805010515.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250805_20250806010501.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250806_20250807010508.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250807_20250808010517.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250808_20250809010515.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250809_20250810010513.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250810_20250811010518.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250811_20250812010502.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250812_20250813010515.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250813_20250814010526.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250814_20250815010506.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250815_20250816010519.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250816_20250817010513.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250817_20250818010504.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250818_20250819010525.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250819_20250820010518.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250820_20250821010519.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250821_20250822010520.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250822_20250823010523.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVPDA3466_ENV_20250823_20250824010500.CSV' ignorado (já inserido ou data inválida).\n",
      "A processar: 20250824\n",
      "Ficheiro 'EVPDA3466_ENV_20250824_20250825010508.CSV' inserido com sucesso (15896 registos). Tempo: 3.44s\n",
      "A processar: 20250825\n",
      "Ficheiro 'EVPDA3466_ENV_20250825_20250826010527.CSV' inserido com sucesso (475183 registos). Tempo: 81.42s\n"
     ]
    }
   ],
   "source": [
    "from ftplib import FTP\n",
    "import os\n",
    "import io\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Conexão MongoDB\n",
    "def conecta_db():\n",
    "    client = MongoClient('mongodb://localhost:27017/')\n",
    "    return client['ctt2025']\n",
    "\n",
    "# Verifica datas já existentes na coleção REDE_BASE_2025\n",
    "def obter_datas_existentes():\n",
    "    db = conecta_db()\n",
    "    colecao = db[\"expresso_2025\"]\n",
    "    datas = colecao.distinct(\"data_criacao\")\n",
    "\n",
    "    regex_data = re.compile(r\"^(\\d{2})/(\\d{2})/(\\d{4})$\")\n",
    "\n",
    "    datas_formatadas = []\n",
    "\n",
    "    for data in datas:\n",
    "        # >>> ALTERAÇÃO (suporta datetime e legado string DD/MM/YYYY)\n",
    "        if isinstance(data, (datetime.date, datetime.datetime)):\n",
    "            y = data.year\n",
    "            m = f\"{data.month:02d}\"\n",
    "            d = f\"{data.day:02d}\"\n",
    "            datas_formatadas.append(f\"{y}{m}{d}\")\n",
    "        elif isinstance(data, str):\n",
    "            match = regex_data.match(data.strip())\n",
    "            if match:\n",
    "                dia, mes, ano = match.groups()\n",
    "                data_formatada = ano + mes.zfill(2) + dia.zfill(2)\n",
    "                datas_formatadas.append(data_formatada)\n",
    "        # <<<\n",
    "\n",
    "    return set(datas_formatadas)\n",
    "\n",
    "# Processa linha\n",
    "def process_data(row):\n",
    "    try:\n",
    "        decoded_line = row.decode('Latin-1')\n",
    "    except UnicodeDecodeError:\n",
    "        decoded_line = row.decode('utf-8')\n",
    "\n",
    "    if len(decoded_line.split(';')) == 13:\n",
    "        return decoded_line.strip()\n",
    "    else:\n",
    "        print(f\"Ignorando linha com colunas incorretas: {decoded_line}\")\n",
    "        return None\n",
    "\n",
    "# Ligar FTP e obter ficheiros\n",
    "ftp = FTP()\n",
    "ftp.connect(host='10.0.25.193', port=2122)\n",
    "ftp.login(user='cax_sgc', passwd='gy768#lo')\n",
    "ftp.cwd('/OP/Rotas/EventosPDA')\n",
    "files = ftp.nlst()\n",
    "ftp.quit()\n",
    "\n",
    "# Obter datas validadas\n",
    "data_validation = obter_datas_existentes()\n",
    "\n",
    "# Processar ficheiros\n",
    "for file in files:\n",
    "    start_time = time.time()\n",
    "    if file.startswith(\"E\"):\n",
    "        date_str_s = file[14:22].replace(\"/\", \"\")\n",
    "        ano = file[14:18]\n",
    "        mes = file[18:20]\n",
    "\n",
    "        if date_str_s not in data_validation and ano == \"2025\" and mes >= \"01\":\n",
    "            print(f\"A processar: {date_str_s}\")\n",
    "\n",
    "            ftp = FTP()\n",
    "            ftp.connect(host='10.0.25.193', port=2122)\n",
    "            ftp.login(user='cax_sgc', passwd='gy768#lo')\n",
    "            ftp.cwd('/OP/Rotas/EventosPDA')\n",
    "\n",
    "            file_data = io.BytesIO()\n",
    "            ftp.retrbinary('RETR ' + file, file_data.write)\n",
    "            file_data.seek(0)\n",
    "\n",
    "            try:\n",
    "                df = pd.read_csv(file_data, sep=';', on_bad_lines='skip', encoding=\"Latin-1\")\n",
    "            except pd.errors.ParserError as pe:\n",
    "                print(f\"Erro ao ler CSV no ficheiro '{file}': {pe}\")\n",
    "                ftp.quit()\n",
    "                continue\n",
    "\n",
    "            ftp.quit()\n",
    "\n",
    "            if df.empty:\n",
    "                print(f\"Ficheiro '{file}' vazio ou com linhas inválidas. Ignorado.\")\n",
    "                continue\n",
    "\n",
    "            # >>> ALTERAÇÃO: nomes de colunas em minúsculas\n",
    "            df.columns = [c.lower() for c in df.columns]\n",
    "            # <<<\n",
    "\n",
    "            # Mantém limpeza de nulos como estava\n",
    "            df = df.where(pd.notna(df), None)\n",
    "\n",
    "            # >>> ALTERAÇÃO: data_criacao como datetime (Date no Mongo)\n",
    "            dia_i = int(date_str_s[6:8])\n",
    "            mes_i = int(mes)\n",
    "            ano_i = int(ano)\n",
    "            data_dt = datetime.datetime(ano_i, mes_i, dia_i)\n",
    "            df[\"data_criacao\"] = data_dt\n",
    "            # <<<\n",
    "\n",
    "            # >>> ALTERAÇÃO: restantes campos como string\n",
    "            for col in df.columns:\n",
    "                if col != \"data_criacao\":\n",
    "                    df[col] = df[col].astype(str)\n",
    "                    df[col] = df[col].replace([\"nan\", \"None\"], \"\")\n",
    "            # <<<\n",
    "\n",
    "            db = conecta_db()\n",
    "            colecao = db['expresso_2025']\n",
    "\n",
    "            try:\n",
    "                documentos = df.to_dict(orient='records')\n",
    "                if documentos:\n",
    "                    colecao.insert_many(documentos)\n",
    "                    print(f\"Ficheiro '{file}' inserido com sucesso ({len(documentos)} registos). Tempo: {time.time() - start_time:.2f}s\")\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao inserir dados do ficheiro '{file}': {e}\")\n",
    "        else:\n",
    "            print(f\"Ficheiro '{file}' ignorado (já inserido ou data inválida).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Banca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coleção 'banca_2025' já existe.\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "from pymongo.errors import PyMongoError\n",
    "\n",
    "# Conexão MongoDB\n",
    "def conecta_db():\n",
    "    client = MongoClient('mongodb://localhost:27017/')\n",
    "    db = client['ctt2025']\n",
    "    return db\n",
    "\n",
    "# Criar coleção (opcional em MongoDB)\n",
    "def criar_colecao(nome_colecao):\n",
    "    db = conecta_db()\n",
    "    if nome_colecao not in db.list_collection_names():\n",
    "        db.create_collection(nome_colecao)\n",
    "        print(f\"Coleção '{nome_colecao}' criada.\")\n",
    "    else:\n",
    "        print(f\"Coleção '{nome_colecao}' já existe.\")\n",
    "\n",
    "# Inserir um documento (linha)\n",
    "def inserir_documento(nome_colecao, documento):\n",
    "    db = conecta_db()\n",
    "    colecao = db[nome_colecao]\n",
    "    try:\n",
    "        colecao.insert_one(documento)\n",
    "        print(\"Documento inserido com sucesso.\")\n",
    "    except PyMongoError as error:\n",
    "        print(f\"Erro ao inserir documento: {error}\")\n",
    "\n",
    "# Criar a coleção (equivalente a \"CREATE TABLE IF NOT EXISTS\")\n",
    "criar_colecao(\"banca_2025\")\n",
    "\n",
    "# Exemplo de inserção (podes adaptar à tua fonte de dados)\n",
    "documento_exemplo = {\n",
    "    \"COD_CLIENTE_BNC\": \"123456\",\n",
    "    \"CODIGO_BALCAO_SIDIR\": \"001\",\n",
    "    \"CODIGO_BALCAO_AF\": \"AF456\",\n",
    "    \"AGENCIA\": \"Lisboa\",\n",
    "    \"CA\": \"Central\",\n",
    "    \"CIRCUITO\": \"C1\",\n",
    "    \"DATA\": \"24/04/2025\",\n",
    "    \"CONTACTO\": \"João Silva\",\n",
    "    \"FATURAR\": \"Sim\",\n",
    "    \"ENTREGUE\": \"Não\",\n",
    "    \"TAREFA\": \"Entrega\",\n",
    "    \"CP\": \"1000-001\",\n",
    "    \"CENTRO_OPERACIONAL\": \"Centro A\",\n",
    "    \"MORADA_TOQUE\": \"Rua das Flores\",\n",
    "    \"HORA_TOQUE_PREVISTA\": \"10:00\",\n",
    "    \"HORA_TOQUE_REAL\": \"10:15\"\n",
    "}\n",
    "\n",
    "# Inserir esse documento na coleção\n",
    "#inserir_documento(\"banca_2025\", documento_exemplo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20250210' '20250211' '20250212' '20250213' '20250214' '20250217'\n",
      " '20250218' '20250219' '20250220' '20250221' '20250224' '20250225'\n",
      " '20250226' '20250227' '20250228' '20250303' '20250304' '20250305'\n",
      " '20250306' '20250307' '20250310' '20250311' '20250312' '20250313'\n",
      " '20250314' '20250317' '20250318' '20250319' '20250320' '20250321'\n",
      " '20250324' '20250325' '20250326' '20250327' '20250328' '20250331'\n",
      " '20250401' '20250402' '20250403' '20250404' '20250407' '20250408'\n",
      " '20250409' '20250410' '20250411' '20250414' '20250415' '20250416'\n",
      " '20250417' '20250418' '20250421' '20250422' '20250423' '20250424'\n",
      " '20250425' '20250428' '20250429' '20250430' '20250501' '20250502'\n",
      " '20250505' '20250506' '20250507' '20250508' '20250509' '20250512'\n",
      " '20250519' '20250520' '20250521' '20250522' '20250523' '20250526'\n",
      " '20250527' '20250528' '20250529' '20250530' '20250602' '20250603'\n",
      " '20250604' '20250605' '20250606' '20250609' '20250610' '20250611'\n",
      " '20250612' '20250613' '20250616' '20250617' '20250618' '20250619'\n",
      " '20250620' '20250623' '20250624' '20250625' '20250626' '20250627'\n",
      " '20250630' '20250724']\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def conecta_db():\n",
    "    client = MongoClient('mongodb://localhost:27017/')\n",
    "    return client['ctt2025']\n",
    "\n",
    "# Conectar à base de dados\n",
    "db = conecta_db()\n",
    "colecao = db['banca_2025']\n",
    "\n",
    "# Obter todas as datas distintas do campo DATA\n",
    "datas = colecao.distinct(\"data\")\n",
    "\n",
    "data_validation = []\n",
    "\n",
    "# Expressão regular correta: YYYY.MM.DD\n",
    "regex_data = re.compile(r\"^(\\d{4})\\.(\\d{1,2})\\.(\\d{1,2})$\")\n",
    "\n",
    "for data in datas:\n",
    "    if isinstance(data, str):\n",
    "        match = regex_data.match(data.strip())\n",
    "        if match:\n",
    "            ano, mes, dia = match.groups()\n",
    "            data_formatada = ano + mes.zfill(2) + dia.zfill(2)\n",
    "            data_validation.append(data_formatada)\n",
    "        else:\n",
    "            print(f\"Formato inválido ignorado: {data}\")\n",
    "\n",
    "# Eliminar duplicados, ordenar e converter para numpy array\n",
    "data_validation = np.array(sorted(set(data_validation)))\n",
    "print(data_validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IGNORADO: CAX019_TOQUE_BANCA_D_20250728_20250729000615.CSV (data: 20250728) — já inserido ou fora do ano alvo.\n",
      "IGNORADO: CAX019_TOQUE_BANCA_D_20250729_20250730000615.CSV (data: 20250729) — já inserido ou fora do ano alvo.\n",
      "IGNORADO: CAX019_TOQUE_BANCA_D_20250730_20250731000623.CSV (data: 20250730) — já inserido ou fora do ano alvo.\n",
      "IGNORADO: CAX019_TOQUE_BANCA_D_20250731_20250801000626.CSV (data: 20250731) — já inserido ou fora do ano alvo.\n",
      "IGNORADO: CAX019_TOQUE_BANCA_D_20250801_20250802000615.CSV (data: 20250801) — já inserido ou fora do ano alvo.\n",
      "IGNORADO: CAX019_TOQUE_BANCA_D_20250804_20250805000624.CSV (data: 20250804) — já inserido ou fora do ano alvo.\n",
      "IGNORADO: CAX019_TOQUE_BANCA_D_20250805_20250806000627.CSV (data: 20250805) — já inserido ou fora do ano alvo.\n",
      "IGNORADO: CAX019_TOQUE_BANCA_D_20250806_20250807000631.CSV (data: 20250806) — já inserido ou fora do ano alvo.\n",
      "IGNORADO: CAX019_TOQUE_BANCA_D_20250807_20250808000656.CSV (data: 20250807) — já inserido ou fora do ano alvo.\n",
      "IGNORADO: CAX019_TOQUE_BANCA_D_20250808_20250809000613.CSV (data: 20250808) — já inserido ou fora do ano alvo.\n",
      "IGNORADO: CAX019_TOQUE_BANCA_D_20250811_20250812000604.CSV (data: 20250811) — já inserido ou fora do ano alvo.\n",
      "IGNORADO: CAX019_TOQUE_BANCA_D_20250812_20250813000611.CSV (data: 20250812) — já inserido ou fora do ano alvo.\n",
      "IGNORADO: CAX019_TOQUE_BANCA_D_20250813_20250814000633.CSV (data: 20250813) — já inserido ou fora do ano alvo.\n",
      "IGNORADO: CAX019_TOQUE_BANCA_D_20250814_20250815000610.CSV (data: 20250814) — já inserido ou fora do ano alvo.\n",
      "IGNORADO: CAX019_TOQUE_BANCA_D_20250815_20250816000614.CSV (data: 20250815) — já inserido ou fora do ano alvo.\n",
      "IGNORADO: CAX019_TOQUE_BANCA_D_20250818_20250819000611.CSV (data: 20250818) — já inserido ou fora do ano alvo.\n",
      "IGNORADO: CAX019_TOQUE_BANCA_D_20250819_20250820000609.CSV (data: 20250819) — já inserido ou fora do ano alvo.\n",
      "IGNORADO: CAX019_TOQUE_BANCA_D_20250820_20250821000606.CSV (data: 20250820) — já inserido ou fora do ano alvo.\n",
      "IGNORADO: CAX019_TOQUE_BANCA_D_20250821_20250822000615.CSV (data: 20250821) — já inserido ou fora do ano alvo.\n",
      "IGNORADO: CAX019_TOQUE_BANCA_D_20250822_20250823000608.CSV (data: 20250822) — já inserido ou fora do ano alvo.\n"
     ]
    }
   ],
   "source": [
    "# Conexão FTP\n",
    "ftp = FTP()\n",
    "ftp.connect(host='10.0.25.193', port=2122)\n",
    "ftp.login(user='cax_sgc', passwd='gy768#lo')\n",
    "ftp.cwd('/OP/Rotas/Banca_Recolhas')\n",
    "files = ftp.nlst()\n",
    "ftp.quit()\n",
    "\n",
    "# Datas já presentes\n",
    "data_validation = obter_datas_existentes()\n",
    "\n",
    "# Processamento\n",
    "for file in files:\n",
    "    start_time = time.time()\n",
    "    if file.startswith(\"C\"):\n",
    "        matches = re.findall(r\"\\d{8}\", file)\n",
    "        if matches:\n",
    "            date_str_s = matches[0]  # usa a primeira data no nome\n",
    "            ano = date_str_s[:4]\n",
    "        else:\n",
    "            print(f\"⚠️ Nome inválido sem data no ficheiro: {file}\")\n",
    "            continue\n",
    "\n",
    "        #print(f\"→ A verificar ficheiro: {file}\")\n",
    "        #print(f\"  Data extraída: {date_str_s}\")\n",
    "        #print(f\"  Está em data_validation? {'SIM' if date_str_s in data_validation else 'NÃO'}\")\n",
    "\n",
    "        if date_str_s not in data_validation and ano == \"2025\":\n",
    "            # Reabrir conexão FTP para o ficheiro\n",
    "            ftp = FTP()\n",
    "            ftp.connect(host='10.0.25.193', port=2122)\n",
    "            ftp.login(user='cax_sgc', passwd='gy768#lo')\n",
    "            ftp.cwd('/OP/Rotas/Banca_Recolhas')\n",
    "\n",
    "            file_data = io.BytesIO()\n",
    "            ftp.retrbinary('RETR ' + file, file_data.write)\n",
    "            file_data.seek(0)\n",
    "            lines = file_data.readlines()\n",
    "            ftp.quit()\n",
    "\n",
    "            def process_data(row):\n",
    "                try:\n",
    "                    return row.decode('Latin-1').strip()\n",
    "                except UnicodeDecodeError:\n",
    "                    return row.decode('utf-8').strip()\n",
    "\n",
    "            data_rows = [process_data(row) for row in lines[:-1]]\n",
    "\n",
    "            if data_rows and data_rows[0].startswith('\\ufeff'):\n",
    "                data_rows[0] = data_rows[0][1:]\n",
    "\n",
    "            csv_data = '\\n'.join(data_rows[:-1]).encode('utf-8')\n",
    "            decoded_data = csv_data.decode('utf-8-sig')\n",
    "\n",
    "            if decoded_data.strip():\n",
    "                df = pd.read_csv(io.StringIO(decoded_data), sep=';', on_bad_lines='skip', encoding=\"Latin-1\")\n",
    "\n",
    "                column_mapping = {\n",
    "                    'ï»¿COD_CLIENTE_BNC': 'COD_CLIENTE_BNC',\n",
    "                    'CODIGO_BALCAO_SIDIR': 'CODIGO_BALCAO_SIDIR',\n",
    "                    'CODIGO_BALCAO_AF': 'CODIGO_BALCAO_AF',\n",
    "                    'AGENCIA': 'AGENCIA',\n",
    "                    'CA': 'CA',\n",
    "                    'CIRCUITO': 'CIRCUITO',\n",
    "                    'DATA': 'DATA',\n",
    "                    'CONTACTO': 'CONTACTO',\n",
    "                    'FATURAR': 'FATURAR',\n",
    "                    'ENTREGUE': 'ENTREGUE',\n",
    "                    'TAREFA': 'TAREFA',\n",
    "                    'CP': 'CP',\n",
    "                    'CENTRO OPERACIONAL': 'CENTRO_OPERACIONAL',\n",
    "                    'MORADA TOQUE': 'MORADA_TOQUE',\n",
    "                    'HORA TOQUE PREVISTA': 'HORA_TOQUE_PREVISTA',\n",
    "                    'HORA TOQUE REAL': 'HORA_TOQUE_REAL'\n",
    "                }\n",
    "\n",
    "                df.rename(columns=column_mapping, inplace=True)\n",
    "                df = df.where(pd.notna(df), None)\n",
    "\n",
    "                # Forçar DATA no formato YYYY.MM.DD\n",
    "                data_formatada = f\"{date_str_s[:4]}.{date_str_s[4:6]}.{date_str_s[6:]}\"\n",
    "                df['data'] = data_formatada\n",
    "                print(f\"→ Será inserida a data: {data_formatada}\")\n",
    "\n",
    "                db = conecta_db()\n",
    "                colecao = db['banca_2025']\n",
    "\n",
    "                try:\n",
    "                    documentos = df.to_dict(orient='records')\n",
    "                    if documentos:\n",
    "                        colecao.insert_many(documentos)\n",
    "                        print(f\"✅ Ficheiro '{file}' inserido com sucesso ({len(documentos)} registos). Tempo: {time.time() - start_time:.2f}s\")\n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Erro ao inserir dados do ficheiro '{file}': {e}\")\n",
    "        else:\n",
    "            print(f\"IGNORADO: {file} (data: {date_str_s}) — já inserido ou fora do ano alvo.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "recolhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coleção 'recolhas_2025' já existe.\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "from pymongo.errors import PyMongoError\n",
    "\n",
    "# Conexão MongoDB\n",
    "def conecta_db():\n",
    "    client = MongoClient('mongodb://localhost:27017/')\n",
    "    db = client['ctt2025']\n",
    "    return db\n",
    "\n",
    "# Criar coleção (opcional — Mongo cria automaticamente ao inserir)\n",
    "def criar_colecao(nome_colecao):\n",
    "    db = conecta_db()\n",
    "    if nome_colecao not in db.list_collection_names():\n",
    "        db.create_collection(nome_colecao)\n",
    "        print(f\"Coleção '{nome_colecao}' criada.\")\n",
    "    else:\n",
    "        print(f\"Coleção '{nome_colecao}' já existe.\")\n",
    "\n",
    "# Inserir documento (linha)\n",
    "def inserir_documento(nome_colecao, documento):\n",
    "    db = conecta_db()\n",
    "    colecao = db[nome_colecao]\n",
    "    try:\n",
    "        colecao.insert_one(documento)\n",
    "        print(\"Documento inserido com sucesso.\")\n",
    "    except PyMongoError as error:\n",
    "        print(f\"Erro ao inserir documento: {error}\")\n",
    "\n",
    "# Criar coleção equivalente à tabela Recolhas_2025\n",
    "criar_colecao(\"recolhas_2025\")\n",
    "\n",
    "# Exemplo de documento (linha)\n",
    "documento_exemplo = {\n",
    "    \"data_ficheiro\": \"2025.04.24\",\n",
    "    \"Código Orgânico\": \"ORG001\",\n",
    "    \"Descrição\": \"Serviço diário\",\n",
    "    \"Dia\": \"Quinta-feira\",\n",
    "    \"Giro\": \"G1\",\n",
    "    \"Tipo\": \"Normal\",\n",
    "    \"Ponto Recolha\": \"Ponto A\",\n",
    "    \"Nome Cliente\": \"Cliente XPTO\",\n",
    "    \"Morada de recolha\": \"Rua das Flores, 10\",\n",
    "    \"Código postal recolha\": \"1000-001\",\n",
    "    \"Qntd de Objetos Recolhidos\": \"12\",\n",
    "    \"Estado\": \"Concluído\",\n",
    "    \"Hora Início\": \"08:00\",\n",
    "    \"Hora Fim\": \"10:00\",\n",
    "    \"Hora Real\": \"09:35\"\n",
    "}\n",
    "\n",
    "# Inserir esse documento na coleção\n",
    "#inserir_documento(\"Recolhas_2025\", documento_exemplo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20250531' '20250601' '20250602' '20250603' '20250604' '20250605'\n",
      " '20250606' '20250607' '20250608' '20250609' '20250610' '20250611'\n",
      " '20250612' '20250613' '20250614' '20250615' '20250616' '20250617'\n",
      " '20250618' '20250619' '20250620' '20250621' '20250622' '20250623'\n",
      " '20250624' '20250625' '20250626' '20250627' '20250628' '20250629'\n",
      " '20250630' '20250701' '20250702' '20250703' '20250704' '20250705'\n",
      " '20250706' '20250707' '20250708' '20250709' '20250710' '20250711'\n",
      " '20250712' '20250713' '20250714' '20250715' '20250716' '20250717'\n",
      " '20250718' '20250719' '20250720' '20250721' '20250722' '20250723'\n",
      " '20250724' '20250725' '20250726' '20250727' '20250728' '20250729'\n",
      " '20250730' '20250731' '20250801' '20250802' '20250803' '20250804'\n",
      " '20250805' '20250806' '20250807' '20250808' '20250809' '20250810'\n",
      " '20250811' '20250812' '20250813' '20250814' '20250815' '20250816'\n",
      " '20250817' '20250818' '20250819' '20250820' '20250821']\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def conecta_db():\n",
    "    client = MongoClient('mongodb://localhost:27017/')\n",
    "    return client['ctt2025']\n",
    "\n",
    "# Conectar à base de dados\n",
    "db = conecta_db()\n",
    "colecao = db['recolhas_2025']\n",
    "\n",
    "# Obter todas as datas distintas do campo data_ficheiro\n",
    "datas = colecao.distinct(\"data_ficheiro\")\n",
    "\n",
    "data_validation = []\n",
    "\n",
    "# Expressão regular para datas no formato YYYY.MM.DD\n",
    "regex_data = re.compile(r\"^(\\d{4})\\.(\\d{2})\\.(\\d{2})$\")\n",
    "\n",
    "\n",
    "\n",
    "for data in datas:\n",
    "    if isinstance(data, str):\n",
    "        match = regex_data.match(data.strip())\n",
    "        if match:\n",
    "            ano, mes, dia = match.groups()\n",
    "            data_formatada = ano + mes.zfill(2) + dia.zfill(2)\n",
    "            data_validation.append(data_formatada)\n",
    "        else:\n",
    "            print(f\"Formato inválido ignorado: {data}\")\n",
    "\n",
    "# Eliminar duplicados, ordenar e converter para numpy array\n",
    "data_validation = np.array(sorted(set(data_validation)))\n",
    "print(data_validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ficheiro 'RECSG4366_RECOLHAS_20250726_20250727010500.CSV' ignorado (já existente ou fora do ano alvo).\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250727_20250728010500.CSV' ignorado (já existente ou fora do ano alvo).\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250728_20250729010500.CSV' ignorado (já existente ou fora do ano alvo).\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250729_20250730010500.CSV' ignorado (já existente ou fora do ano alvo).\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250730_20250731010501.CSV' ignorado (já existente ou fora do ano alvo).\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250731_20250801010500.CSV' ignorado (já existente ou fora do ano alvo).\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250801_20250802010501.CSV' ignorado (já existente ou fora do ano alvo).\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250802_20250803010500.CSV' ignorado (já existente ou fora do ano alvo).\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250803_20250804010500.CSV' ignorado (já existente ou fora do ano alvo).\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250804_20250805010501.CSV' ignorado (já existente ou fora do ano alvo).\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250805_20250806010501.CSV' ignorado (já existente ou fora do ano alvo).\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250806_20250807010500.CSV' ignorado (já existente ou fora do ano alvo).\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250807_20250808010501.CSV' ignorado (já existente ou fora do ano alvo).\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250808_20250809010501.CSV' ignorado (já existente ou fora do ano alvo).\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250809_20250810010500.CSV' ignorado (já existente ou fora do ano alvo).\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250810_20250811010500.CSV' ignorado (já existente ou fora do ano alvo).\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250811_20250812010500.CSV' ignorado (já existente ou fora do ano alvo).\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250812_20250813010500.CSV' ignorado (já existente ou fora do ano alvo).\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250813_20250814010500.CSV' ignorado (já existente ou fora do ano alvo).\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250814_20250815010501.CSV' ignorado (já existente ou fora do ano alvo).\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250815_20250816010500.CSV' ignorado (já existente ou fora do ano alvo).\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250816_20250817010501.CSV' ignorado (já existente ou fora do ano alvo).\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250817_20250818010500.CSV' ignorado (já existente ou fora do ano alvo).\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250818_20250819010500.CSV' ignorado (já existente ou fora do ano alvo).\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250819_20250820010500.CSV' ignorado (já existente ou fora do ano alvo).\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250820_20250821010501.CSV' ignorado (já existente ou fora do ano alvo).\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250821_20250822010500.CSV' ignorado (já existente ou fora do ano alvo).\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250822_20250823010500.CSV' inserido com sucesso (12414 registos). Tempo: 2.84s\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250823_20250824010500.CSV' inserido com sucesso (4418 registos). Tempo: 1.42s\n",
      "Ficheiro 'RECSG4366_RECOLHAS_20250824_20250825010500.CSV' inserido com sucesso (4504 registos). Tempo: 1.03s\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import time\n",
    "from ftplib import FTP\n",
    "from pymongo import MongoClient\n",
    "import re\n",
    "\n",
    "# Conexão MongoDB\n",
    "def conecta_db():\n",
    "    client = MongoClient('mongodb://localhost:27017/')\n",
    "    return client['ctt2025']\n",
    "\n",
    "# Obter datas já existentes em MongoDB (Recolhas_2025)\n",
    "def obter_datas_existentes():\n",
    "    db = conecta_db()\n",
    "    colecao = db[\"recolhas_2025\"]\n",
    "    datas = colecao.distinct(\"data_ficheiro\")\n",
    "\n",
    "    regex_data = re.compile(r\"^(\\d{4})\\.(\\d{2})\\.(\\d{2})$\")\n",
    "\n",
    "\n",
    "    datas_formatadas = []\n",
    "\n",
    "    for data in datas:\n",
    "        if isinstance(data, str):\n",
    "            match = regex_data.match(data.strip())\n",
    "            if match:\n",
    "                ano, mes, dia = match.groups()\n",
    "                data_formatada = ano + mes + dia\n",
    "                datas_formatadas.append(data_formatada)\n",
    "            else:\n",
    "                print(f\"Formato inválido ignorado: {data}\")\n",
    "\n",
    "    return set(datas_formatadas)\n",
    "\n",
    "\n",
    "# Processamento de cada linha do ficheiro\n",
    "def process_data(row):\n",
    "    try:\n",
    "        return row.decode('Latin-1').strip()\n",
    "    except UnicodeDecodeError:\n",
    "        return row.decode('utf-8').strip()\n",
    "\n",
    "# Nomes das colunas\n",
    "columns = ['\"Código Orgânico\"', '\"Descrição\"', '\"Dia\"', '\"Giro\"', '\"Tipo\"', '\"Ponto Recolha\"', '\"Nome Cliente\"',\n",
    "           '\"Morada de recolha\"', '\"Código postal recolha\"', '\"Qntd de Objetos Recolhidos\"', '\"Estado\"', '\"Hora Início\"',\n",
    "           '\"Hora Fim\"', '\"Hora Real\"']\n",
    "\n",
    "# Obter lista de ficheiros do FTP\n",
    "ftp = FTP()\n",
    "ftp.connect(host='10.0.25.193', port=2122)\n",
    "ftp.login(user='cax_sgc', passwd='gy768#lo')\n",
    "ftp.cwd('/OP/Rotas/Banca_Recolhas')\n",
    "files = ftp.nlst()\n",
    "ftp.quit()\n",
    "\n",
    "# Datas já presentes no MongoDB\n",
    "data_validation = obter_datas_existentes()\n",
    "\n",
    "# Processar ficheiros\n",
    "for file in files:\n",
    "    start_time = time.time()\n",
    "    if file.startswith(\"R\"):\n",
    "        date_str_s = file[19:27].replace(\"/\", \"\")\n",
    "        ano = file[19:23]\n",
    "\n",
    "        if date_str_s not in data_validation and ano == \"2025\":\n",
    "            # Nova ligação FTP para o ficheiro\n",
    "            ftp = FTP()\n",
    "            ftp.connect(host='10.0.25.193', port=2122)\n",
    "            ftp.login(user='cax_sgc', passwd='gy768#lo')\n",
    "            ftp.cwd('/OP/Rotas/Banca_Recolhas')\n",
    "\n",
    "            file_data = io.BytesIO()\n",
    "            ftp.retrbinary('RETR ' + file, file_data.write)\n",
    "            file_data.seek(0)\n",
    "            lines = file_data.readlines()\n",
    "            data_rows = [process_data(row) for row in lines[:-1]]\n",
    "            ftp.quit()\n",
    "\n",
    "            if data_rows and data_rows[0].startswith('\\ufeff'):\n",
    "                data_rows[0] = data_rows[0][1:]\n",
    "\n",
    "            csv_data = '\\n'.join(data_rows[:-1]).encode('utf-8')\n",
    "            decoded_data = csv_data.decode('utf-8-sig')\n",
    "\n",
    "            if decoded_data.strip():\n",
    "                df = pd.read_csv(io.StringIO(decoded_data), sep=';', names=columns, on_bad_lines='skip', encoding=\"Latin-1\")\n",
    "                df = df.iloc[1:]  # remover header duplicado\n",
    "                df['data_ficheiro'] = f\"{ano}.{date_str_s[4:6]}.{date_str_s[6:]}\"\n",
    "                df = df.where(pd.notna(df), None)\n",
    "\n",
    "                db = conecta_db()\n",
    "                colecao = db['recolhas_2025']\n",
    "\n",
    "                try:\n",
    "                    documentos = df.to_dict(orient='records')\n",
    "                    if documentos:\n",
    "                        colecao.insert_many(documentos)\n",
    "                        print(f\"Ficheiro '{file}' inserido com sucesso ({len(documentos)} registos). Tempo: {time.time() - start_time:.2f}s\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Erro ao inserir dados do ficheiro '{file}': {e}\")\n",
    "        else:\n",
    "            print(f\"Ficheiro '{file}' ignorado (já existente ou fora do ano alvo).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rede base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coleção 'rede_base_2025' já existe.\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "from pymongo.errors import PyMongoError\n",
    "\n",
    "# Conexão MongoDB\n",
    "def conecta_db():\n",
    "    client = MongoClient('mongodb://localhost:27017/')\n",
    "    return client['ctt2025']\n",
    "\n",
    "# Criar coleção (opcional — Mongo cria automaticamente ao inserir)\n",
    "def criar_colecao(nome_colecao):\n",
    "    db = conecta_db()\n",
    "    if nome_colecao not in db.list_collection_names():\n",
    "        db.create_collection(nome_colecao)\n",
    "        print(f\"Coleção '{nome_colecao}' criada.\")\n",
    "    else:\n",
    "        print(f\"Coleção '{nome_colecao}' já existe.\")\n",
    "\n",
    "# Inserir documento na coleção\n",
    "def inserir_documento(nome_colecao, documento):\n",
    "    db = conecta_db()\n",
    "    colecao = db[nome_colecao]\n",
    "\n",
    "    # Verificação para evitar duplicados com base na DATA_CRIACAO\n",
    "    data_chave = documento.get(\"data_criacao\")\n",
    "    if data_chave:\n",
    "        existe = colecao.find_one({\"data_criacao\": data_chave})\n",
    "        if existe:\n",
    "            print(f\"Documento com DATA_CRIACAO '{data_chave}' já existe. Ignorado.\")\n",
    "            return\n",
    "\n",
    "    try:\n",
    "        colecao.insert_one(documento)\n",
    "        print(\"Documento inserido com sucesso.\")\n",
    "    except PyMongoError as error:\n",
    "        print(f\"Erro ao inserir documento: {error}\")\n",
    "\n",
    "# Criar coleção\n",
    "criar_colecao(\"rede_base_2025\")\n",
    "\n",
    "# Exemplo de documento\n",
    "documento_exemplo = {\n",
    "    \"DATA_CRIACAO\": \"2025.04.24\",\n",
    "    \"CENTRO\": \"Centro Norte\",\n",
    "    \"Giro\": \"G45\",\n",
    "    \"LOPTICA\": \"L1234\",\n",
    "    \"JANELA_HORARIA\": \"08:00-10:00\",\n",
    "    \"NOME\": \"Loja XPTO\",\n",
    "    \"MORADA\": \"Rua Exemplo, 100\",\n",
    "    \"CP\": \"4000-123\",\n",
    "    \"LOCALIDADE\": \"Porto\",\n",
    "    \"COD_T_EVEN\": \"EV123\",\n",
    "    \"DATA_EVENTO\": \"2025.04.23\",\n",
    "    \"LATITUDE\": \"41.14961\",\n",
    "    \"LONGITUDE\": \"-8.61099\",\n",
    "    \"NOME_REM\": \"Remetente ABC\",\n",
    "    \"COD_PAIS_ORIGEM\": \"PT\"\n",
    "}\n",
    "\n",
    "# Inserir documento\n",
    "#inserir_documento(\"REDE_BASE_2025\", documento_exemplo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20250210' '20250211' '20250212' '20250213' '20250214' '20250215'\n",
      " '20250217' '20250218' '20250219' '20250220' '20250221' '20250222'\n",
      " '20250224' '20250225' '20250226' '20250227' '20250228' '20250301'\n",
      " '20250303' '20250304' '20250305' '20250306' '20250307' '20250308'\n",
      " '20250310' '20250325' '20250326' '20250327' '20250328' '20250329'\n",
      " '20250331' '20250401' '20250402' '20250403' '20250404' '20250405'\n",
      " '20250407' '20250408' '20250409' '20250410' '20250411' '20250412'\n",
      " '20250413' '20250414' '20250415' '20250416' '20250417' '20250418'\n",
      " '20250419' '20250421' '20250422' '20250423' '20250424' '20250425'\n",
      " '20250426' '20250428' '20250429' '20250430' '20250501' '20250502'\n",
      " '20250503' '20250505' '20250506' '20250507' '20250508' '20250509'\n",
      " '20250510' '20250512' '20250513' '20250514' '20250515' '20250516'\n",
      " '20250517' '20250519' '20250520' '20250521' '20250522' '20250523'\n",
      " '20250524' '20250526' '20250527' '20250528' '20250529' '20250530'\n",
      " '20250531' '20250601' '20250602' '20250603' '20250604' '20250605'\n",
      " '20250606' '20250607' '20250608' '20250609' '20250610' '20250611'\n",
      " '20250612' '20250613' '20250614' '20250615' '20250616' '20250617'\n",
      " '20250618' '20250619' '20250620' '20250621' '20250622' '20250623'\n",
      " '20250624' '20250625' '20250626' '20250627' '20250628' '20250629'\n",
      " '20250630' '20250701' '20250702' '20250703' '20250704' '20250705'\n",
      " '20250706' '20250707' '20250708' '20250709' '20250710' '20250711'\n",
      " '20250712' '20250713' '20250714' '20250715' '20250716' '20250717'\n",
      " '20250718' '20250719' '20250720' '20250721' '20250722' '20250723'\n",
      " '20250724' '20250725' '20250726' '20250727' '20250728' '20250729'\n",
      " '20250730' '20250731' '20250801' '20250802' '20250803' '20250804'\n",
      " '20250805' '20250806' '20250807' '20250808' '20250809' '20250810'\n",
      " '20250811' '20250812' '20250813' '20250814' '20250815' '20250816'\n",
      " '20250817' '20250818' '20250819' '20250820' '20250821']\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def conecta_db():\n",
    "    client = MongoClient('mongodb://localhost:27017/')\n",
    "    return client['ctt2025']\n",
    "\n",
    "# Conectar à base de dados\n",
    "db = conecta_db()\n",
    "colecao = db['rede_base_2025']\n",
    "\n",
    "# Obter todas as datas distintas do campo DATA_CRIACAO\n",
    "datas = colecao.distinct(\"data_criacao\")\n",
    "\n",
    "data_validation = []\n",
    "\n",
    "# Expressão regular para datas no formato YYYY.MM.DD\n",
    "regex_data = re.compile(r\"^(\\d{2})/(\\d{2})/(\\d{4})$\")\n",
    "\n",
    "for data in datas:\n",
    "    if isinstance(data, str):\n",
    "        match = regex_data.match(data.strip())\n",
    "        if match:\n",
    "            dia, mes, ano = match.groups()\n",
    "            data_formatada = ano + mes.zfill(2) + dia.zfill(2)\n",
    "            data_validation.append(data_formatada)\n",
    "        else:\n",
    "            print(f\"Formato inválido ignorado: {data}\")\n",
    "\n",
    "# Eliminar duplicados, ordenar e converter para numpy array\n",
    "data_validation = np.array(sorted(set(data_validation)))\n",
    "print(data_validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ficheiro 'EVRBP4348_ENV_20250726_20250727010521.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVRBP4348_ENV_20250727_20250728010524.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVRBP4348_ENV_20250728_20250729010501.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVRBP4348_ENV_20250729_20250730010517.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVRBP4348_ENV_20250730_20250731010503.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVRBP4348_ENV_20250731_20250801010504.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVRBP4348_ENV_20250801_20250802010514.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVRBP4348_ENV_20250802_20250803010501.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVRBP4348_ENV_20250803_20250804010506.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVRBP4348_ENV_20250804_20250805010515.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVRBP4348_ENV_20250805_20250806010501.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVRBP4348_ENV_20250806_20250807010508.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVRBP4348_ENV_20250807_20250808010517.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVRBP4348_ENV_20250808_20250809010516.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVRBP4348_ENV_20250809_20250810010513.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVRBP4348_ENV_20250810_20250811010518.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVRBP4348_ENV_20250811_20250812010502.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVRBP4348_ENV_20250812_20250813010515.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVRBP4348_ENV_20250813_20250814010526.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVRBP4348_ENV_20250814_20250815010506.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVRBP4348_ENV_20250815_20250816010519.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVRBP4348_ENV_20250816_20250817010513.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVRBP4348_ENV_20250817_20250818010504.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVRBP4348_ENV_20250818_20250819010525.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVRBP4348_ENV_20250819_20250820010518.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVRBP4348_ENV_20250820_20250821010519.CSV' ignorado (já inserido ou data inválida).\n",
      "Ficheiro 'EVRBP4348_ENV_20250821_20250822010520.CSV' ignorado (já inserido ou data inválida).\n",
      "A processar: 20250822\n",
      "Ficheiro 'EVRBP4348_ENV_20250822_20250823010523.CSV' inserido com sucesso (283354 registos). Tempo: 73.28s\n",
      "A processar: 20250823\n",
      "Ficheiro 'EVRBP4348_ENV_20250823_20250824010500.CSV' inserido com sucesso (8375 registos). Tempo: 3.49s\n",
      "A processar: 20250824\n",
      "Ficheiro 'EVRBP4348_ENV_20250824_20250825010508.CSV' inserido com sucesso (16 registos). Tempo: 0.36s\n"
     ]
    }
   ],
   "source": [
    "from ftplib import FTP\n",
    "import os\n",
    "import io\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Conexão MongoDB\n",
    "def conecta_db():\n",
    "    client = MongoClient('mongodb://localhost:27017/')\n",
    "    return client['ctt2025']\n",
    "\n",
    "# Verifica datas já existentes na coleção REDE_BASE_2025\n",
    "def obter_datas_existentes():\n",
    "    db = conecta_db()\n",
    "    colecao = db[\"rede_base_2025\"]\n",
    "    datas = colecao.distinct(\"data_criacao\")\n",
    "\n",
    "    regex_data = re.compile(r\"^(\\d{2})/(\\d{2})/(\\d{4})$\")\n",
    "    datas_formatadas = []\n",
    "\n",
    "    for data in datas:\n",
    "        if isinstance(data, str):\n",
    "            match = regex_data.match(data.strip())\n",
    "            if match:\n",
    "                dia, mes, ano = match.groups()\n",
    "                data_formatada = ano + mes.zfill(2) + dia.zfill(2)\n",
    "                datas_formatadas.append(data_formatada)\n",
    "    \n",
    "    return set(datas_formatadas)\n",
    "\n",
    "# Processa linha\n",
    "def process_data(row):\n",
    "    try:\n",
    "        decoded_line = row.decode('Latin-1')\n",
    "    except UnicodeDecodeError:\n",
    "        decoded_line = row.decode('utf-8')\n",
    "\n",
    "    if len(decoded_line.split(';')) == 13:\n",
    "        return decoded_line.strip()\n",
    "    else:\n",
    "        print(f\"Ignorando linha com colunas incorretas: {decoded_line}\")\n",
    "        return None\n",
    "\n",
    "# Ligar FTP e obter ficheiros\n",
    "ftp = FTP()\n",
    "ftp.connect(host='10.0.25.193', port=2122)\n",
    "ftp.login(user='cax_sgc', passwd='gy768#lo')\n",
    "ftp.cwd('/OP/Rotas/EventosPDA_Rede_base')\n",
    "files = ftp.nlst()\n",
    "ftp.quit()\n",
    "\n",
    "# Obter datas validadas\n",
    "data_validation = obter_datas_existentes()\n",
    "\n",
    "# Processar ficheiros\n",
    "for file in files:\n",
    "    start_time = time.time()\n",
    "    if file.startswith(\"E\"):\n",
    "        date_str_s = file[14:22].replace(\"/\", \"\")\n",
    "        ano = file[14:18]\n",
    "        mes = file[18:20]\n",
    "\n",
    "        if date_str_s not in data_validation and ano == \"2025\" and mes >= \"01\":\n",
    "            print(f\"A processar: {date_str_s}\")\n",
    "\n",
    "            ftp = FTP()\n",
    "            ftp.connect(host='10.0.25.193', port=2122)\n",
    "            ftp.login(user='cax_sgc', passwd='gy768#lo')\n",
    "            ftp.cwd('/OP/Rotas/EventosPDA_Rede_base')\n",
    "\n",
    "            file_data = io.BytesIO()\n",
    "            ftp.retrbinary('RETR ' + file, file_data.write)\n",
    "            file_data.seek(0)\n",
    "\n",
    "            try:\n",
    "                df = pd.read_csv(file_data, sep=';', on_bad_lines='skip', encoding=\"Latin-1\")\n",
    "            except pd.errors.ParserError as pe:\n",
    "                print(f\"Erro ao ler CSV no ficheiro '{file}': {pe}\")\n",
    "                ftp.quit()\n",
    "                continue\n",
    "\n",
    "            ftp.quit()\n",
    "\n",
    "            if df.empty:\n",
    "                print(f\"Ficheiro '{file}' vazio ou com linhas inválidas. Ignorado.\")\n",
    "                continue\n",
    "\n",
    "            df = df.where(pd.notna(df), None)\n",
    "            df[\"data_criacao\"] = f\"{date_str_s[6:]}/{mes}/{ano}\"\n",
    "\n",
    "\n",
    "            db = conecta_db()\n",
    "            colecao = db['rede_base_2025']\n",
    "\n",
    "            try:\n",
    "                documentos = df.to_dict(orient='records')\n",
    "                if documentos:\n",
    "                    colecao.insert_many(documentos)\n",
    "                    print(f\"Ficheiro '{file}' inserido com sucesso ({len(documentos)} registos). Tempo: {time.time() - start_time:.2f}s\")\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao inserir dados do ficheiro '{file}': {e}\")\n",
    "        else:\n",
    "            print(f\"Ficheiro '{file}' ignorado (já inserido ou data inválida).\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
