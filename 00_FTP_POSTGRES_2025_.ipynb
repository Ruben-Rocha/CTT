{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "def conecta_db():\n",
    "      con = psycopg2.connect(host='localhost', \n",
    "                         database='ctt2025',\n",
    "                         user='postgres', \n",
    "                         password='1983')\n",
    "      return con\n",
    "\n",
    "\n",
    "def criar_db(sql):\n",
    "    con = conecta_db()\n",
    "    cur = con.cursor()\n",
    "    cur.execute(sql)\n",
    "    con.commit()\n",
    "    con.close()\n",
    "\n",
    "\n",
    "def inserir_db(sql):\n",
    "    con = conecta_db()\n",
    "    cur = con.cursor()\n",
    "    try:\n",
    "        cur.execute(sql)\n",
    "        con.commit()\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(\"Error: %s\" % error)\n",
    "        con.rollback()\n",
    "        cur.close()\n",
    "        return 1\n",
    "    cur.close()\n",
    "\n",
    "sql = ''' CREATE TABLE IF NOT EXISTS Expresso_2025 (\n",
    "                                    DATA_CRIACAO      text,\n",
    "                                    CENTRO    text,\n",
    "                                    Giro text,\n",
    "                                    LOPTICA  text,\n",
    "                                    JANELA_HORARIA text,\n",
    "                                    NOME text,\n",
    "                                    MORADA text,\n",
    "                                    CP text,\n",
    "                                    LOCALIDADE text,\n",
    "                                    COD_T_EVEN text,\n",
    "                                    DATA_EVENTO text,\n",
    "                                    LATITUDE text,\n",
    "                                    LONGITUDE text,\n",
    "                                    NOME_REM text,\n",
    "                                    COD_PAIS_ORIGEM text) '''\n",
    "criar_db(sql)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20250621' '20250622' '20250623' '20250624' '20250625' '20250626'\n",
      " '20250627' '20250628' '20250629' '20250630' '20250701' '20250702'\n",
      " '20250703' '20250704' '20250705' '20250706' '20250707' '20250708'\n",
      " '20250709' '20250731' '20250801' '20250802' '20250803' '20250804'\n",
      " '20250805' '20250806' '20250807' '20250808' '20250809' '20250810'\n",
      " '20250811' '20250812' '20250813' '20250814' '20250815' '20250816'\n",
      " '20250817' '20250818' '20250819' '20250820' '20250821' '20250822'\n",
      " '20250823' '20250824' '20250825' '20250826' '20250827' '20250828'\n",
      " '20250829' '20250830' '20250831' '20250901' '20250902' '20250903'\n",
      " '20250904' '20250905' '20250906' '20250907' '20250908' '20250909'\n",
      " '20250910' '20250911' '20250912' '20250913' '20250914']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "data_validation = []\n",
    "con = conecta_db()\n",
    "cur = con.cursor()\n",
    "cur.execute(\"SELECT DISTINCT CONCAT(SUBSTRING(DATA_CRIACAO, 7, 4),SUBSTRING(DATA_CRIACAO, 4, 2),SUBSTRING(DATA_CRIACAO, 1, 2)) AS ConvertedDate FROM Expresso_2025\")\n",
    "for record in cur.fetchall():\n",
    "    if record[0] not in data_validation:\n",
    "        data_validation.append(record[0])\n",
    "cur.close()\n",
    "\n",
    "data_validation=np.array(data_validation)\n",
    "print(data_validation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expresso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'EVPDA3466_ENV_20250915_20250916010528.CSV' inserted into the database: 472.05556535720825 seconds\n",
      "File 'EVPDA3466_ENV_20250916_20250917010521.CSV' inserted into the database: 507.25663590431213 seconds\n",
      "File 'EVPDA3466_ENV_20250917_20250918010502.CSV' inserted into the database: 509.08722448349 seconds\n",
      "File 'EVPDA3466_ENV_20250918_20250919010516.CSV' inserted into the database: 478.04343938827515 seconds\n",
      "File 'EVPDA3466_ENV_20250919_20250920010504.CSV' inserted into the database: 470.49392104148865 seconds\n",
      "File 'EVPDA3466_ENV_20250920_20250921010509.CSV' inserted into the database: 95.78235006332397 seconds\n",
      "File 'EVPDA3466_ENV_20250921_20250922010515.CSV' inserted into the database: 20.585904121398926 seconds\n"
     ]
    }
   ],
   "source": [
    "from ftplib import FTP\n",
    "import os\n",
    "import io\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "import time\n",
    "\n",
    "current_year = datetime.datetime.now().year\n",
    "\n",
    "ftp = FTP()\n",
    "ftp.connect(host='10.0.25.193', port=2122)\n",
    "ftp.login(user='cax_sgc', passwd='gy768#lo')\n",
    "\n",
    "ftp.cwd('/OP/Rotas/EventosPDA')\n",
    "\n",
    "files = ftp.nlst()\n",
    "\n",
    "def process_data(row):\n",
    "    try:\n",
    "        decoded_line = row.decode('Latin-1')\n",
    "    except UnicodeDecodeError as e:\n",
    "        print(f\"UnicodeDecodeError in file '{file}', line: {row}\")\n",
    "        decoded_line = row.decode('utf-8')\n",
    "    return decoded_line.strip()\n",
    "\n",
    "data_validation = []\n",
    "\n",
    "cur = con.cursor()\n",
    "\n",
    "cur.execute(\"SELECT distinct  SUBSTRING(DATA_CRIACAO, 7, 4) ||  SUBSTRING(DATA_CRIACAO, 4, 2) ||  SUBSTRING(DATA_CRIACAO, 1, 2) as dias FROM Expresso_2025 order by dias asc\")\n",
    "for record in cur.fetchall():\n",
    "    if record[0] not in data_validation:\n",
    "        data_validation.append(record[0])\n",
    "\n",
    "cur.close()\n",
    "\n",
    "for file in files:\n",
    "    start_time = time.time()\n",
    "    if file.startswith(\"E\"):\n",
    "        date_str_s = file[14:22].replace(\"/\", \"\")\n",
    "        ano = file[14:18]\n",
    "        mes = date_str_s[4:6]\n",
    "        date_str_s_array = np.array(date_str_s)\n",
    "        \n",
    "        if date_str_s not in data_validation and ano == \"2025\" and mes >= \"05\":\n",
    "            \n",
    "            ftp = FTP()\n",
    "            ftp.connect(host='10.0.25.193', port=2122)\n",
    "            ftp.login(user='cax_sgc', passwd='gy768#lo')\n",
    "            ftp.cwd('/OP/Rotas/EventosPDA')\n",
    "            \n",
    "            file_data = io.BytesIO()\n",
    "            ftp.retrbinary('RETR ' + file, file_data.write)\n",
    "            file_data.seek(0)  # Reset the stream position to the beginning\n",
    "\n",
    "            # Process the data line by line, excluding the last line\n",
    "            lines = file_data.readlines()\n",
    "            data_rows = [process_data(row) for row in lines[:-1]]  # Exclude the last line\n",
    "            df = pd.read_csv(io.StringIO('\\n'.join(data_rows)), sep=';', on_bad_lines='skip', encoding=\"Latin-1\")\n",
    "            ftp.quit()\n",
    "            \n",
    "            if len(data_rows) == 1: \n",
    "                continue\n",
    "            else:\n",
    "                # Insert the data into the database\n",
    "                for index, row in df.iterrows():\n",
    "                    try:\n",
    "                        cur = con.cursor()\n",
    "                        # Replace \"NaN\" values with None (NULL) for database insertion\n",
    "                        row = [None if pd.isna(value) else value for value in row]\n",
    "                        placeholders = ', '.join(['%s'] * len(row))\n",
    "                        insert_query = f\"INSERT INTO Expresso_2025 ({', '.join(df.columns)}) VALUES ({placeholders})\"\n",
    "                        cur.execute(insert_query, tuple(row))\n",
    "                    except (Exception, psycopg2.DatabaseError) as error:\n",
    "                        print(f\"Error inserting row {index+1}: {error}\")\n",
    "                        con.rollback()\n",
    "                    else:\n",
    "                        con.commit()\n",
    "                        \n",
    "                end_time = time.time()    \n",
    "                print(f\"File '{file}' inserted into the database: {end_time - start_time} seconds\")\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom ftplib import FTP\\nimport io\\nimport pandas as pd\\nimport numpy as np\\nimport psycopg2\\nimport psycopg2.extras\\nimport time\\n\\n# --- ligações pré-existentes ---\\n# con = psycopg2.connect(...)\\n\\n# 1) validar dias já carregados (usa set para O(1))\\nwith con.cursor() as cur:\\n    cur.execute(\"\"\"\\n        SELECT DISTINCT SUBSTRING(DATA_CRIACAO, 7, 4) || SUBSTRING(DATA_CRIACAO, 4, 2) || SUBSTRING(DATA_CRIACAO, 1, 2) AS dias\\n        FROM Expresso_2025\\n    \"\"\")\\n    data_validation = {r[0] for r in cur.fetchall()}\\n\\n# 2) abrir uma única sessão FTP\\nftp = FTP()\\nftp.connect(host=\\'10.0.25.193\\', port=2122)\\nftp.login(user=\\'cax_sgc\\', passwd=\\'gy768#lo\\')\\nftp.cwd(\\'/OP/Rotas/EventosPDA\\')\\nfiles = ftp.nlst()\\n\\ndef process_data(row):\\n    try:\\n        return row.decode(\\'Latin-1\\').strip()\\n    except UnicodeDecodeError:\\n        return row.decode(\\'utf-8\\').strip()\\n\\ndef copy_df(conn, df, table):\\n    # COPY CSV para Postgres (muito mais rápido que INSERT por linha)\\n    buf = io.StringIO()\\n    # Se o teu CSV no destino deve usar \\';\\', mantém sep=\\';\\'\\n    df.to_csv(buf, index=False, header=False, sep=\\';\\')\\n    buf.seek(0)\\n    with conn.cursor() as c:\\n        c.copy_expert(f\"COPY {table} ({\\', \\'.join(df.columns)}) FROM STDIN WITH (FORMAT csv, DELIMITER \\';\\')\", buf)\\n\\n# 3) loop de ficheiros\\nfor file in files:\\n    if not file.startswith(\"E\"):\\n        continue\\n\\n    date_str_s = file[14:22].replace(\"/\", \"\")\\n    ano = file[14:18]\\n    mes = date_str_s[4:6]\\n\\n    # filtros rápidos para evitar trabalho desnecessário\\n    if not (ano == \"2025\" and mes >= \"05\"):\\n        continue\\n    if date_str_s in data_validation:\\n        continue\\n\\n    start_time = time.time()\\n\\n    # ler ficheiro do FTP uma única vez\\n    file_data = io.BytesIO()\\n    ftp.retrbinary(\\'RETR \\' + file, file_data.write)\\n    file_data.seek(0)\\n\\n    # processar linhas (ignorando a última se for trailer)\\n    lines = file_data.readlines()\\n    if len(lines) <= 1:\\n        continue\\n    data_rows = [process_data(row) for row in lines[:-1]]\\n\\n    # construir DataFrame (um parse por ficheiro)\\n    df = pd.read_csv(io.StringIO(\\'\\n\\'.join(data_rows)), sep=\\';\\', on_bad_lines=\\'skip\\', encoding=\"Latin-1\")\\n\\n    # normalizar NaN -> vazio (COPY trata bem) ou, se precisares de NULL, substitui por \\'\\'\\n    df = df.where(pd.notna(df), None)\\n\\n    # 4) transação por ficheiro + COPY\\n    try:\\n        con.autocommit = False\\n        copy_df(con, df, \"Expresso_2025\")\\n        con.commit()\\n        data_validation.add(date_str_s)  # marca como carregado\\n        print(f\"File \\'{file}\\' -> {len(df)} linhas, {time.time() - start_time:.1f}s\")\\n    except Exception as e:\\n        con.rollback()\\n        print(f\"ERRO em \\'{file}\\': {e}\")\\n\\n# fechar FTP e DB\\nftp.quit()\\ncon.close()\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from ftplib import FTP\n",
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "import time\n",
    "\n",
    "# --- ligações pré-existentes ---\n",
    "# con = psycopg2.connect(...)\n",
    "\n",
    "# 1) validar dias já carregados (usa set para O(1))\n",
    "with con.cursor() as cur:\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT DISTINCT SUBSTRING(DATA_CRIACAO, 7, 4) || SUBSTRING(DATA_CRIACAO, 4, 2) || SUBSTRING(DATA_CRIACAO, 1, 2) AS dias\n",
    "        FROM Expresso_2025\n",
    "    \"\"\")\n",
    "    data_validation = {r[0] for r in cur.fetchall()}\n",
    "\n",
    "# 2) abrir uma única sessão FTP\n",
    "ftp = FTP()\n",
    "ftp.connect(host='10.0.25.193', port=2122)\n",
    "ftp.login(user='cax_sgc', passwd='gy768#lo')\n",
    "ftp.cwd('/OP/Rotas/EventosPDA')\n",
    "files = ftp.nlst()\n",
    "\n",
    "def process_data(row):\n",
    "    try:\n",
    "        return row.decode('Latin-1').strip()\n",
    "    except UnicodeDecodeError:\n",
    "        return row.decode('utf-8').strip()\n",
    "\n",
    "def copy_df(conn, df, table):\n",
    "    # COPY CSV para Postgres (muito mais rápido que INSERT por linha)\n",
    "    buf = io.StringIO()\n",
    "    # Se o teu CSV no destino deve usar ';', mantém sep=';'\n",
    "    df.to_csv(buf, index=False, header=False, sep=';')\n",
    "    buf.seek(0)\n",
    "    with conn.cursor() as c:\n",
    "        c.copy_expert(f\"COPY {table} ({', '.join(df.columns)}) FROM STDIN WITH (FORMAT csv, DELIMITER ';')\", buf)\n",
    "\n",
    "# 3) loop de ficheiros\n",
    "for file in files:\n",
    "    if not file.startswith(\"E\"):\n",
    "        continue\n",
    "\n",
    "    date_str_s = file[14:22].replace(\"/\", \"\")\n",
    "    ano = file[14:18]\n",
    "    mes = date_str_s[4:6]\n",
    "\n",
    "    # filtros rápidos para evitar trabalho desnecessário\n",
    "    if not (ano == \"2025\" and mes >= \"05\"):\n",
    "        continue\n",
    "    if date_str_s in data_validation:\n",
    "        continue\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # ler ficheiro do FTP uma única vez\n",
    "    file_data = io.BytesIO()\n",
    "    ftp.retrbinary('RETR ' + file, file_data.write)\n",
    "    file_data.seek(0)\n",
    "\n",
    "    # processar linhas (ignorando a última se for trailer)\n",
    "    lines = file_data.readlines()\n",
    "    if len(lines) <= 1:\n",
    "        continue\n",
    "    data_rows = [process_data(row) for row in lines[:-1]]\n",
    "\n",
    "    # construir DataFrame (um parse por ficheiro)\n",
    "    df = pd.read_csv(io.StringIO('\\n'.join(data_rows)), sep=';', on_bad_lines='skip', encoding=\"Latin-1\")\n",
    "\n",
    "    # normalizar NaN -> vazio (COPY trata bem) ou, se precisares de NULL, substitui por ''\n",
    "    df = df.where(pd.notna(df), None)\n",
    "\n",
    "    # 4) transação por ficheiro + COPY\n",
    "    try:\n",
    "        con.autocommit = False\n",
    "        copy_df(con, df, \"Expresso_2025\")\n",
    "        con.commit()\n",
    "        data_validation.add(date_str_s)  # marca como carregado\n",
    "        print(f\"File '{file}' -> {len(df)} linhas, {time.time() - start_time:.1f}s\")\n",
    "    except Exception as e:\n",
    "        con.rollback()\n",
    "        print(f\"ERRO em '{file}': {e}\")\n",
    "\n",
    "# fechar FTP e DB\n",
    "ftp.quit()\n",
    "con.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BANCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "def conecta_db():\n",
    "      con = psycopg2.connect(host='localhost', \n",
    "                         database='ctt2025',\n",
    "                         user='postgres', \n",
    "                         password='1983')\n",
    "      return con\n",
    "\n",
    "\n",
    "def criar_db(sql):\n",
    "    con = conecta_db()\n",
    "    cur = con.cursor()\n",
    "    cur.execute(sql)\n",
    "    con.commit()\n",
    "    con.close()\n",
    "\n",
    "\n",
    "def inserir_db(sql):\n",
    "    con = conecta_db()\n",
    "    cur = con.cursor()\n",
    "    try:\n",
    "        cur.execute(sql)\n",
    "        con.commit()\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(\"Error: %s\" % error)\n",
    "        con.rollback()\n",
    "        cur.close()\n",
    "        return 1\n",
    "    cur.close()\n",
    "\n",
    "sql = ''' CREATE TABLE IF NOT EXISTS BANCA_2025 (\n",
    "                                    COD_CLIENTE_BNC text,\n",
    "                                    CODIGO_BALCAO_SIDIR text,\n",
    "                                    CODIGO_BALCAO_AF text,\n",
    "                                    AGENCIA text,\n",
    "                                    CA text,\n",
    "                                    CIRCUITO text,\n",
    "                                    DATA text,\n",
    "                                    CONTACTO text,\n",
    "                                    FATURAR text,\n",
    "                                    ENTREGUE text,\n",
    "                                    TAREFA text,\n",
    "                                    CP text,\n",
    "                                    CENTRO_OPERACIONAL text,\n",
    "                                    MORADA_TOQUE text,\n",
    "                                    HORA_TOQUE_PREVISTA text,\n",
    "                                    HORA_TOQUE_REAL text\n",
    "                                    ) '''\n",
    "criar_db(sql)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20250624' '20250625' '20250626' '20250627' '20250630' '20250701'\n",
      " '20250702' '20250703' '20250704' '20250707' '20250708' '20250709'\n",
      " '20250710' '20250711' '20250714' '20250715' '20250716' '20250717'\n",
      " '20250718' '20250721' '20250722' '20250723' '20250724' '20250725'\n",
      " '20250728' '20250729' '20250730' '20250731' '20250801' '20250804'\n",
      " '20250805' '20250806' '20250807' '20250808' '20250811' '20250812'\n",
      " '20250813' '20250814' '20250815' '20250818' '20250819' '20250820'\n",
      " '20250821' '20250822' '20250825' '20250826' '20250827' '20250828'\n",
      " '20250829' '20250901' '20250902' '20250903' '20250904' '20250905'\n",
      " '20250908' '20250909' '20250910' '20250911' '20250912']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "data_validation = []\n",
    "con = conecta_db()\n",
    "cur = con.cursor(cursor_factory=psycopg2.extras.DictCursor)\n",
    "cur.execute(\"SELECT distinct REPLACE(data, '.', '') dias FROM Banca_2025 order by dias asc\")\n",
    "for record in cur.fetchall():\n",
    "    if record[0] not in data_validation:\n",
    "        data_validation.append(record[0])\n",
    "cur.close()\n",
    "\n",
    "data_validation=np.array(data_validation)\n",
    "print(data_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'CAX019_TOQUE_BANCA_D_20250915_20250916000609.CSV' inserted into the database: 0.8097207546234131 seconds\n",
      "File 'CAX019_TOQUE_BANCA_D_20250916_20250917000608.CSV' inserted into the database: 1.8296308517456055 seconds\n",
      "File 'CAX019_TOQUE_BANCA_D_20250917_20250918000626.CSV' inserted into the database: 5.0165910720825195 seconds\n",
      "File 'CAX019_TOQUE_BANCA_D_20250918_20250919000615.CSV' inserted into the database: 0.7191054821014404 seconds\n",
      "File 'CAX019_TOQUE_BANCA_D_20250919_20250920000607.CSV' inserted into the database: 0.9407622814178467 seconds\n"
     ]
    }
   ],
   "source": [
    "from ftplib import FTP\n",
    "import os\n",
    "import io\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "import time\n",
    "\n",
    "current_year = datetime.datetime.now().year\n",
    "\n",
    "ftp = FTP()\n",
    "ftp.connect(host='10.0.25.193', port=2122)\n",
    "ftp.login(user='cax_sgc', passwd='gy768#lo')\n",
    "\n",
    "ftp.cwd('/OP/Rotas/Banca_Recolhas')\n",
    "\n",
    "files = ftp.nlst()\n",
    "\n",
    "def process_data(row):\n",
    "    try:\n",
    "        decoded_line = row.decode('Latin-1')\n",
    "    except UnicodeDecodeError as e:\n",
    "        print(f\"UnicodeDecodeError in file '{file}', line: {row}\")\n",
    "        decoded_line = row.decode('utf-8')\n",
    "    return decoded_line.strip()\n",
    "\n",
    "data_validation = []\n",
    "\n",
    "con = psycopg2.connect(host='localhost', \n",
    "                    database='ctt2025',\n",
    "                    user='postgres', \n",
    "                    password='1983')\n",
    "\n",
    "cur = con.cursor()\n",
    "\n",
    "cur.execute(\"SELECT distinct REPLACE(data, '.', '') dias FROM Banca_2025 order by dias asc\")\n",
    "for record in cur.fetchall():\n",
    "    if record[0] not in data_validation:\n",
    "        data_validation.append(record[0])\n",
    "\n",
    "cur.close()\n",
    "\n",
    "# Create a mapping between input column names and database column names\n",
    "column_mapping = {\n",
    "    'ï»¿COD_CLIENTE_BNC': 'COD_CLIENTE_BNC',\n",
    "    'CODIGO_BALCAO_SIDIR': 'CODIGO_BALCAO_SIDIR',\n",
    "    'CODIGO_BALCAO_AF': 'CODIGO_BALCAO_AF',\n",
    "    'AGENCIA': 'AGENCIA',\n",
    "    'CA': 'CA',\n",
    "    'CIRCUITO': 'CIRCUITO',\n",
    "    'DATA': 'DATA',\n",
    "    'CONTACTO': 'CONTACTO',\n",
    "    'FATURAR': 'FATURAR',\n",
    "    'ENTREGUE': 'ENTREGUE',\n",
    "    'TAREFA': 'TAREFA',\n",
    "    'CP': 'CP',\n",
    "    'CENTRO OPERACIONAL': 'CENTRO_OPERACIONAL',\n",
    "    'MORADA TOQUE': 'MORADA_TOQUE',\n",
    "    'HORA TOQUE PREVISTA': 'HORA_TOQUE_PREVISTA',\n",
    "    'HORA TOQUE REAL': 'HORA_TOQUE_REAL'\n",
    "}\n",
    "\n",
    "for file in files:\n",
    "    start_time = time.time()\n",
    "    if file.startswith(\"C\"):\n",
    "        date_str_s = file[21:29].replace(\"/\", \"\")\n",
    "        date_str_s_array = np.array(date_str_s)\n",
    "        ano = file[21:25]\n",
    "        if date_str_s not in data_validation and ano ==\"2025\":\n",
    "            \n",
    "            ftp = FTP()\n",
    "            ftp.connect(host='10.0.25.193', port=2122)\n",
    "            ftp.login(user='cax_sgc', passwd='gy768#lo')\n",
    "            ftp.cwd('/OP/Rotas/Banca_Recolhas')\n",
    "            \n",
    "            file_data = io.BytesIO()\n",
    "            ftp.retrbinary('RETR ' + file, file_data.write)\n",
    "            file_data.seek(0)  # Reset the stream position to the beginning\n",
    "\n",
    "            # Process the data line by line, excluding the last line\n",
    "            lines = file_data.readlines()\n",
    "            data_rows = [process_data(row) for row in lines[:-1]]  # Exclude the last line\n",
    "            \n",
    "            # Remove BOM if present\n",
    "            if data_rows and data_rows[0].startswith('\\ufeff'):\n",
    "                data_rows[0] = data_rows[0][1:]\n",
    "            \n",
    "            # Join the data_rows without the last line (to exclude the empty line) and convert to bytes\n",
    "            csv_data = '\\n'.join(data_rows[:-1]).encode('utf-8')\n",
    "            \n",
    "            # Decode the bytes data using utf-8-sig to handle the BOM\n",
    "            decoded_data = csv_data.decode('utf-8-sig')\n",
    "            \n",
    "            if decoded_data.strip():  # Check if the decoded data is not empty\n",
    "                # Create a DataFrame from the cleaned data rows\n",
    "                df = pd.read_csv(io.StringIO(decoded_data), sep=';', on_bad_lines='skip', encoding=\"Latin-1\")\n",
    "\n",
    "                # Rename the DataFrame columns based on the mapping\n",
    "                df.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "                # Insert the data into the database\n",
    "                try:\n",
    "                    cur = con.cursor()\n",
    "                    # Replace \"NaN\" values with None (NULL) for database insertion\n",
    "                    df = df.where(pd.notna(df), None)\n",
    "\n",
    "                    # Generate placeholders for the query based on the number of columns\n",
    "                    placeholders = ', '.join(['%s'] * len(df.columns))\n",
    "\n",
    "                    # Construct the INSERT query with placeholders\n",
    "                    insert_query = f\"INSERT INTO Banca_2025 ({', '.join(df.columns)}) VALUES ({placeholders})\"\n",
    "\n",
    "                    # Execute the query with the DataFrame values as parameters\n",
    "                    psycopg2.extras.execute_batch(cur, insert_query, df.values)\n",
    "                    con.commit()\n",
    "\n",
    "                    end_time = time.time()\n",
    "                    print(f\"File '{file}' inserted into the database: {end_time - start_time} seconds\")\n",
    "                except (Exception, psycopg2.DatabaseError) as error:\n",
    "                    print(f\"Error inserting data from file '{file}': {error}\")\n",
    "                    con.rollback()\n",
    "\n",
    "ftp.quit()\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recolhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "def conecta_db():\n",
    "      con = psycopg2.connect(host='localhost', \n",
    "                         database='ctt2025',\n",
    "                         user='postgres', \n",
    "                         password='1983')\n",
    "      return con\n",
    "\n",
    "\n",
    "def criar_db(sql):\n",
    "    con = conecta_db()\n",
    "    cur = con.cursor()\n",
    "    cur.execute(sql)\n",
    "    con.commit()\n",
    "    con.close()\n",
    "\n",
    "\n",
    "def inserir_db(sql):\n",
    "    con = conecta_db()\n",
    "    cur = con.cursor()\n",
    "    try:\n",
    "        cur.execute(sql)\n",
    "        con.commit()\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(\"Error: %s\" % error)\n",
    "        con.rollback()\n",
    "        cur.close()\n",
    "        return 1\n",
    "    cur.close()\n",
    "\n",
    "sql = '''CREATE TABLE IF NOT EXISTS Recolhas_2025 (\n",
    "            \"data_ficheiro\" text,\n",
    "            \"Código Orgânico\" text,\n",
    "            \"Descrição\" text,\n",
    "            \"Dia\" text,\n",
    "            \"Giro\" text,\n",
    "            \"Tipo\" text,\n",
    "            \"Ponto Recolha\" text,\n",
    "            \"Nome Cliente\" text,\n",
    "            \"Morada de recolha\" text,\n",
    "            \"Código postal recolha\" text,\n",
    "            \"Qntd de Objetos Recolhidos\" text,\n",
    "            \"Estado\" text,\n",
    "            \"Hora Início\" text,\n",
    "            \"Hora Fim\" text,\n",
    "            \"Hora Real\" text\n",
    "        )'''\n",
    "\n",
    "\n",
    "criar_db(sql)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20250624' '20250625' '20250626' '20250627' '20250628' '20250629'\n",
      " '20250630' '20250701' '20250702' '20250703' '20250704' '20250705'\n",
      " '20250706' '20250707' '20250708' '20250709' '20250710' '20250711'\n",
      " '20250712' '20250713' '20250714' '20250715' '20250716' '20250717'\n",
      " '20250718' '20250719' '20250720' '20250721' '20250722' '20250723'\n",
      " '20250724' '20250725' '20250726' '20250727' '20250728' '20250729'\n",
      " '20250730' '20250731' '20250801' '20250802' '20250803' '20250804'\n",
      " '20250805' '20250806' '20250807' '20250808' '20250809' '20250810'\n",
      " '20250811' '20250812' '20250813' '20250814' '20250815' '20250816'\n",
      " '20250817' '20250818' '20250819' '20250820' '20250821' '20250822'\n",
      " '20250823' '20250824' '20250825' '20250826' '20250827' '20250828'\n",
      " '20250829' '20250830' '20250831' '20250901' '20250902' '20250903'\n",
      " '20250904' '20250905' '20250906' '20250907' '20250908' '20250909'\n",
      " '20250910' '20250911' '20250912' '20250913' '20250914']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "data_validation = []\n",
    "con = conecta_db()\n",
    "cur = con.cursor(cursor_factory=psycopg2.extras.DictCursor)\n",
    "\n",
    "cur.execute(\"SELECT DISTINCT  data_ficheiro as dias FROM Recolhas_2025  ORDER BY DIAS ASC\")\n",
    "for record in cur.fetchall():\n",
    "    if record[0] not in data_validation:\n",
    "        data_validation.append(record[0])\n",
    "cur.close()\n",
    "\n",
    "data_validation=np.array(data_validation)\n",
    "print(data_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'RECSG4366_RECOLHAS_20250915_20250916010500.CSV' inserted into the database: 7.778303146362305 seconds\n",
      "File 'RECSG4366_RECOLHAS_20250916_20250917010500.CSV' inserted into the database: 3.9348695278167725 seconds\n",
      "File 'RECSG4366_RECOLHAS_20250917_20250918010500.CSV' inserted into the database: 5.893521785736084 seconds\n",
      "File 'RECSG4366_RECOLHAS_20250918_20250919010500.CSV' inserted into the database: 9.078545808792114 seconds\n",
      "File 'RECSG4366_RECOLHAS_20250919_20250920010500.CSV' inserted into the database: 4.830184459686279 seconds\n",
      "File 'RECSG4366_RECOLHAS_20250920_20250921010500.CSV' inserted into the database: 2.043255567550659 seconds\n",
      "File 'RECSG4366_RECOLHAS_20250921_20250922010500.CSV' inserted into the database: 1.0502777099609375 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import time\n",
    "from ftplib import FTP\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "import datetime\n",
    "\n",
    "\n",
    "current_year = datetime.datetime.now().year\n",
    "\n",
    "ftp = FTP()\n",
    "ftp.connect(host='10.0.25.193', port=2122)\n",
    "ftp.login(user='cax_sgc', passwd='gy768#lo')\n",
    "\n",
    "ftp.cwd('/OP/Rotas/Banca_Recolhas')\n",
    "\n",
    "files = ftp.nlst()\n",
    "\n",
    "def process_data(row):\n",
    "    try:\n",
    "        decoded_line = row.decode('Latin-1')\n",
    "    except UnicodeDecodeError as e:\n",
    "        print(f\"UnicodeDecodeError in file '{file}', line: {row}\")\n",
    "        decoded_line = row.decode('utf-8')\n",
    "    return decoded_line.strip()\n",
    "\n",
    "data_validation = []\n",
    "\n",
    "con = psycopg2.connect(host='localhost', \n",
    "                    database='ctt2025',\n",
    "                    user='postgres', \n",
    "                    password='1983')\n",
    "\n",
    "cur = con.cursor()\n",
    "\n",
    "cur.execute(\"SELECT DISTINCT  data_ficheiro as dias FROM Recolhas_2025  ORDER BY DIAS ASC\")\n",
    "\n",
    "for record in cur.fetchall():\n",
    "    if record[0] not in data_validation:\n",
    "        data_validation.append(record[0])\n",
    "\n",
    "# List of column names\n",
    "columns = ['\"Código Orgânico\"', '\"Descrição\"', '\"Dia\"', '\"Giro\"', '\"Tipo\"', '\"Ponto Recolha\"', '\"Nome Cliente\"',\n",
    "           '\"Morada de recolha\"', '\"Código postal recolha\"', '\"Qntd de Objetos Recolhidos\"', '\"Estado\"', '\"Hora Início\"',\n",
    "           '\"Hora Fim\"', '\"Hora Real\"']\n",
    "\n",
    "# Assuming you have a 'files' list defined earlier\n",
    "for file in files:\n",
    "    start_time = time.time()\n",
    "    if file.startswith(\"R\"):\n",
    "        date_str_s = file[19:27].replace(\"/\", \"\")\n",
    "        date_str_s_array = np.array(date_str_s)\n",
    "        ano = file[19:23]\n",
    "        if date_str_s not in data_validation and ano == \"2025\":\n",
    "\n",
    "            ftp = FTP()\n",
    "            ftp.connect(host='10.0.25.193', port=2122)\n",
    "            ftp.login(user='cax_sgc', passwd='gy768#lo')\n",
    "            ftp.cwd('/OP/Rotas/Banca_Recolhas')\n",
    "\n",
    "            file_data = io.BytesIO()\n",
    "            ftp.retrbinary('RETR ' + file, file_data.write)\n",
    "            file_data.seek(0)  # Reset the stream position to the beginning\n",
    "\n",
    "            # Process the data line by line, excluding the last line\n",
    "            lines = file_data.readlines()\n",
    "            data_rows = [process_data(row) for row in lines[:-1]]  # Exclude the last line\n",
    "\n",
    "            # Remove BOM if present\n",
    "            if data_rows and data_rows[0].startswith('\\ufeff'):\n",
    "                data_rows[0] = data_rows[0][1:]\n",
    "\n",
    "            # Join the data_rows without the last line (to exclude the empty line) and convert to bytes\n",
    "            csv_data = '\\n'.join(data_rows[:-1]).encode('utf-8')\n",
    "\n",
    "            # Decode the bytes data using utf-8-sig to handle the BOM\n",
    "            decoded_data = csv_data.decode('utf-8-sig')\n",
    "\n",
    "            if decoded_data.strip():  # Check if the decoded data is not empty\n",
    "                # Create a DataFrame from the cleaned data rows, specifying column names\n",
    "                df = pd.read_csv(io.StringIO(decoded_data), sep=';', names=columns, on_bad_lines='skip', encoding=\"Latin-1\")\n",
    "                df = df.iloc[1:]\n",
    "\n",
    "                # Add the 'data_ficheiro' column and fill it with date_str_s value\n",
    "                df['data_ficheiro'] = date_str_s\n",
    "\n",
    "                # Insert the data into the database\n",
    "                try:\n",
    "                    # Replace \"NaN\" values with None (NULL) for database insertion\n",
    "                    df = df.where(pd.notna(df), None)\n",
    "\n",
    "                    # Generate placeholders for the query based on the number of columns\n",
    "                    placeholders = ', '.join(['%s'] * len(df.columns))\n",
    "\n",
    "                    # Construct the INSERT query with placeholders\n",
    "                    insert_query = f\"INSERT INTO Recolhas_2025 ({', '.join(df.columns)}) VALUES ({placeholders})\"\n",
    "\n",
    "                    # Execute the query with the DataFrame values as parameters\n",
    "                    psycopg2.extras.execute_batch(cur, insert_query, df.values)\n",
    "                    con.commit()\n",
    "\n",
    "                    end_time = time.time()\n",
    "                    print(f\"File '{file}' inserted into the database: {end_time - start_time} seconds\")\n",
    "                except (Exception, psycopg2.DatabaseError) as error:\n",
    "                    print(f\"Error inserting data from file '{file}': {error}\")\n",
    "                    con.rollback()\n",
    "\n",
    "\n",
    "ftp.quit()\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REDE BASE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "def conecta_db():\n",
    "      con = psycopg2.connect(host='localhost', \n",
    "                         database='ctt2025',\n",
    "                         user='postgres', \n",
    "                         password='1983')\n",
    "      return con\n",
    "\n",
    "\n",
    "def criar_db(sql):\n",
    "    con = conecta_db()\n",
    "    cur = con.cursor()\n",
    "    cur.execute(sql)\n",
    "    con.commit()\n",
    "    con.close()\n",
    "\n",
    "\n",
    "def inserir_db(sql):\n",
    "    con = conecta_db()\n",
    "    cur = con.cursor()\n",
    "    try:\n",
    "        cur.execute(sql)\n",
    "        con.commit()\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(\"Error: %s\" % error)\n",
    "        con.rollback()\n",
    "        cur.close()\n",
    "        return 1\n",
    "    cur.close()\n",
    "\n",
    "sql = ''' CREATE TABLE IF NOT EXISTS REDE_BASE_2025 (\n",
    "                                    DATA_CRIACAO      text,\n",
    "                                    CENTRO    text,\n",
    "                                    Giro text,\n",
    "                                    LOPTICA  text,\n",
    "                                    JANELA_HORARIA text,\n",
    "                                    NOME text,\n",
    "                                    MORADA text,\n",
    "                                    CP text,\n",
    "                                    LOCALIDADE text,\n",
    "                                    COD_T_EVEN text,\n",
    "                                    DATA_EVENTO text,\n",
    "                                    LATITUDE text,\n",
    "                                    LONGITUDE text,\n",
    "                                    NOME_REM text,\n",
    "                                    COD_PAIS_ORIGEM text) '''\n",
    "criar_db(sql)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20250624' '20250625' '20250626' '20250627' '20250628' '20250630'\n",
      " '20250701' '20250702' '20250703' '20250704' '20250705' '20250707'\n",
      " '20250708' '20250709' '20250710' '20250711' '20250712' '20250713'\n",
      " '20250714' '20250715' '20250716' '20250717' '20250718' '20250719'\n",
      " '20250720' '20250721' '20250722' '20250723' '20250724' '20250725'\n",
      " '20250726' '20250727' '20250728' '20250729' '20250730' '20250731'\n",
      " '20250801' '20250802' '20250803' '20250804' '20250805' '20250806'\n",
      " '20250807' '20250808' '20250809' '20250810' '20250811' '20250812'\n",
      " '20250813' '20250814' '20250815' '20250816' '20250817' '20250818'\n",
      " '20250819' '20250820' '20250821' '20250822' '20250823' '20250824'\n",
      " '20250825' '20250826' '20250827' '20250828' '20250829' '20250830'\n",
      " '20250831' '20250901' '20250902' '20250903' '20250904' '20250905'\n",
      " '20250906' '20250908' '20250909' '20250910' '20250911' '20250912'\n",
      " '20250913' '20250914' 'NTCORO']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "data_validation = []\n",
    "con = conecta_db()\n",
    "cur = con.cursor(cursor_factory=psycopg2.extras.DictCursor)\n",
    "cur.execute(\"SELECT distinct  SUBSTRING(DATA_CRIACAO, 7, 4) ||  SUBSTRING(DATA_CRIACAO, 4, 2) ||  SUBSTRING(DATA_CRIACAO, 1, 2) as dias FROM REDE_BASE_2025 order by dias asc\")\n",
    "for record in cur.fetchall():\n",
    "    if record[0] not in data_validation:\n",
    "        data_validation.append(record[0])\n",
    "cur.close()\n",
    "\n",
    "data_validation=np.array(data_validation)\n",
    "print(data_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20250907\n",
      "File 'EVRBP4348_ENV_20250907_20250908010515.CSV' inserted into the database: 0.3140735626220703 seconds\n",
      "20250915\n",
      "File 'EVRBP4348_ENV_20250915_20250916010528.CSV' inserted into the database: 278.45496916770935 seconds\n",
      "20250916\n",
      "File 'EVRBP4348_ENV_20250916_20250917010521.CSV' inserted into the database: 274.6365785598755 seconds\n",
      "20250917\n",
      "File 'EVRBP4348_ENV_20250917_20250918010502.CSV' inserted into the database: 311.2449643611908 seconds\n",
      "20250918\n",
      "File 'EVRBP4348_ENV_20250918_20250919010516.CSV' inserted into the database: 317.646071434021 seconds\n",
      "20250919\n",
      "File 'EVRBP4348_ENV_20250919_20250920010504.CSV' inserted into the database: 340.49602127075195 seconds\n",
      "20250920\n",
      "File 'EVRBP4348_ENV_20250920_20250921010509.CSV' inserted into the database: 11.809904336929321 seconds\n",
      "20250921\n",
      "File 'EVRBP4348_ENV_20250921_20250922010515.CSV' inserted into the database: 2.5879428386688232 seconds\n"
     ]
    }
   ],
   "source": [
    "from ftplib import FTP\n",
    "import os\n",
    "import io\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "import time\n",
    "\n",
    "current_year = datetime.datetime.now().year\n",
    "\n",
    "ftp = FTP()\n",
    "ftp.connect(host='10.0.25.193', port=2122)\n",
    "ftp.login(user='cax_sgc', passwd='gy768#lo')\n",
    "\n",
    "ftp.cwd('/OP/Rotas/EventosPDA_Rede_base')\n",
    "\n",
    "files = ftp.nlst()\n",
    "\n",
    "def process_data(row):\n",
    "    try:\n",
    "        decoded_line = row.decode('Latin-1')\n",
    "    except UnicodeDecodeError as e:\n",
    "        print(f\"UnicodeDecodeError in file '{file}', line: {row}\")\n",
    "        decoded_line = row.decode('utf-8')\n",
    "\n",
    "    # Check if the number of columns is 13 (assuming semicolon as the delimiter)\n",
    "    if len(decoded_line.split(';')) == 13:\n",
    "        return decoded_line.strip()\n",
    "    else:\n",
    "        print(f\"Ignoring row with incorrect number of columns: {decoded_line}\")\n",
    "        return None  # Skip the row if it has an incorrect number of columns\n",
    "\n",
    "data_validation = []\n",
    "\n",
    "cur = con.cursor()\n",
    "\n",
    "cur.execute(\"SELECT distinct  SUBSTRING(DATA_CRIACAO, 7, 4) ||  SUBSTRING(DATA_CRIACAO, 4, 2) ||  SUBSTRING(DATA_CRIACAO, 1, 2) as dias FROM REDE_BASE_2025 order by dias asc\")\n",
    "for record in cur.fetchall():\n",
    "    if record[0] not in data_validation:\n",
    "        data_validation.append(record[0])\n",
    "\n",
    "cur.close()\n",
    "\n",
    "for file in files:\n",
    "    start_time = time.time()\n",
    "    if file.startswith(\"E\"):\n",
    "        date_str_s = file[14:22].replace(\"/\", \"\")\n",
    "        month = file[18:20].replace(\"/\", \"\")\n",
    "        date_str_s = np.array(date_str_s)\n",
    "        ano = file[14:18]\n",
    "        if date_str_s not in data_validation and month >= \"01\" and ano == \"2025\":\n",
    "            print(date_str_s)\n",
    "\n",
    "            ftp = FTP()\n",
    "            ftp.connect(host='10.0.25.193', port=2122)\n",
    "            ftp.login(user='cax_sgc', passwd='gy768#lo')\n",
    "            ftp.cwd('/OP/Rotas/EventosPDA_Rede_base')\n",
    "\n",
    "            file_data = io.BytesIO()\n",
    "            ftp.retrbinary('RETR ' + file, file_data.write)\n",
    "            file_data.seek(0)  # Reset the stream position to the beginning\n",
    "\n",
    "            try:\n",
    "                df = pd.read_csv(file_data, sep=';', on_bad_lines='skip', encoding=\"Latin-1\")\n",
    "            except pd.errors.ParserError as pe:\n",
    "                print(f\"ParserError occurred while reading CSV in file '{file}':\")\n",
    "                print(pe)\n",
    "                print(f\"Skipping file '{file}'.\")\n",
    "                continue\n",
    "\n",
    "            ftp.quit()\n",
    "\n",
    "            if len(df) == 0:\n",
    "                print(f\"All rows in file '{file}' were problematic. Skipping insertion.\")\n",
    "                continue\n",
    "            else:\n",
    "                # Create a DataFrame from the cleaned data rows\n",
    "\n",
    "                # Insert the data into the database\n",
    "                for index, row in df.iterrows():\n",
    "                    try:\n",
    "                        cur = con.cursor()\n",
    "                        # Replace \"NaN\" values with None (NULL) for database insertion\n",
    "                        row = [None if pd.isna(value) else value for value in row]\n",
    "                        placeholders = ', '.join(['%s'] * len(row))\n",
    "                        insert_query = f\"INSERT INTO REDE_BASE_2025 ({', '.join(df.columns)}) VALUES ({placeholders})\"\n",
    "                        cur.execute(insert_query, tuple(row))\n",
    "                    except (Exception, psycopg2.DatabaseError) as error:\n",
    "                        print(f\"Error inserting row {index+1} from file '{file}': {error}\")\n",
    "                        con.rollback()\n",
    "                    else:\n",
    "                        con.commit()\n",
    "\n",
    "                end_time = time.time()\n",
    "                print(f\"File '{file}' inserted into the database: {end_time - start_time} seconds\")\n",
    "\n",
    "con.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
