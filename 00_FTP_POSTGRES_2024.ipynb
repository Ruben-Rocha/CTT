{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "def conecta_db():\n",
    "      con = psycopg2.connect(host='localhost', \n",
    "                         database='ctt2024',\n",
    "                         user='postgres', \n",
    "                         password='postgres')\n",
    "      return con\n",
    "\n",
    "\n",
    "def criar_db(sql):\n",
    "    con = conecta_db()\n",
    "    cur = con.cursor()\n",
    "    cur.execute(sql)\n",
    "    con.commit()\n",
    "    con.close()\n",
    "\n",
    "\n",
    "def inserir_db(sql):\n",
    "    con = conecta_db()\n",
    "    cur = con.cursor()\n",
    "    try:\n",
    "        cur.execute(sql)\n",
    "        con.commit()\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(\"Error: %s\" % error)\n",
    "        con.rollback()\n",
    "        cur.close()\n",
    "        return 1\n",
    "    cur.close()\n",
    "\n",
    "sql = ''' CREATE TABLE IF NOT EXISTS Expresso_2024 (\n",
    "                                    DATA_CRIACAO      text,\n",
    "                                    CENTRO    text,\n",
    "                                    Giro text,\n",
    "                                    LOPTICA  text,\n",
    "                                    JANELA_HORARIA text,\n",
    "                                    NOME text,\n",
    "                                    MORADA text,\n",
    "                                    CP text,\n",
    "                                    LOCALIDADE text,\n",
    "                                    COD_T_EVEN text,\n",
    "                                    DATA_EVENTO text,\n",
    "                                    LATITUDE text,\n",
    "                                    LONGITUDE text,\n",
    "                                    NOME_REM text,\n",
    "                                    COD_PAIS_ORIGEM text) '''\n",
    "criar_db(sql)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20240102' '20240103' '20240104' '20240105' '20240106' '20240107'\n",
      " '20240108' '20240109' '20240110' '20240111' '20240112' '20240113'\n",
      " '20240114' '20240115' '20240116' '20240117' '20240118' '20240119'\n",
      " '20240120' '20240121' '20240122' '20240123' '20240124' '20240125'\n",
      " '20240126' '20240127' '20240128' '20240129' '20240130' '20240131'\n",
      " '20240201' '20240202' '20240203' '20240204' '20240205' '20240206'\n",
      " '20240207' '20240208' '20240209' '20240210' '20240211' '20240212'\n",
      " '20240213' '20240214' '20240215' '20240216' '20240217' '20240218'\n",
      " '20240219' '20240220' '20240221' '20240222' '20240223' '20240224'\n",
      " '20240225' '20240226' '20240227' '20240228' '20240229' '20240301'\n",
      " '20240302' '20240303' '20240304' '20240305' '20240306' '20240307'\n",
      " '20240308' '20240309' '20240310' '20240311' '20240312' '20240313'\n",
      " '20240314' '20240315' '20240316' '20240317' '20240318' '20240319'\n",
      " '20240320' '20240321' '20240322' '20240323' '20240324' '20240325'\n",
      " '20240326' '20240327' '20240328' '20240329' '20240330' '20240401'\n",
      " '20240402' '20240403' '20240404' '20240405' '20240406' '20240407'\n",
      " '20240408' '20240409' '20240410' '20240411' '20240412' '20240413'\n",
      " '20240414' '20240415' '20240416' '20240417' '20240418' '20240419'\n",
      " '20240420' '20240421' '20240422' '20240423' '20240424' '20240426'\n",
      " '20240427' '20240428' '20240429' '20240430' '20240501' '20240502'\n",
      " '20240503' '20240504' '20240505' '20240506' '20240507' '20240508'\n",
      " '20240509' '20240510' '20240511' '20240512' '20240513' '20240514'\n",
      " '20240515' '20240516' '20240517' '20240518' '20240519' '20240520'\n",
      " '20240521' '20240522' '20240523' '20240524' '20240525' '20240526'\n",
      " '20240527' '20240528' '20240529' '20240530' '20240531' '20240601'\n",
      " '20240602' '20240603' '20240604' '20240605' '20240606' '20240607'\n",
      " '20240608' '20240609' '20240610' '20240611' '20240612' '20240613'\n",
      " '20240614' '20240615' '20240616' '20240617' '20240618' '20240619'\n",
      " '20240620' '20240621' '20240623' '20240624' '20240625' '20240626'\n",
      " '20240627' '20240628' '20240629' '20240630' '20240701' '20240702'\n",
      " '20240703' '20240704' '20240705' '20240706' '20240707' '20240708'\n",
      " '20240709' '20240710' '20240711' '20240712' '20240713' '20240714'\n",
      " '20240715' '20240716' '20240717' '20240718' '20240719' '20240720'\n",
      " '20240721' '20240722' '20240723' '20240724' '20240725' '20240726'\n",
      " '20240727' '20240728' '20240729' '20240730' '20240731' '20240801'\n",
      " '20240802' '20240803' '20240804' '20240805' '20240806' '20240807'\n",
      " '20240808' '20240809' '20240810' '20240811' '20240812' '20240813'\n",
      " '20240814' '20240815' '20240816' '20240817' '20240818' '20240819']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "data_validation = []\n",
    "con = conecta_db()\n",
    "cur = con.cursor()\n",
    "cur.execute(\"SELECT DISTINCT CONCAT(SUBSTRING(DATA_CRIACAO, 7, 4),SUBSTRING(DATA_CRIACAO, 4, 2),SUBSTRING(DATA_CRIACAO, 1, 2)) AS ConvertedDate FROM Expresso_2024\")\n",
    "for record in cur.fetchall():\n",
    "    if record[0] not in data_validation:\n",
    "        data_validation.append(record[0])\n",
    "cur.close()\n",
    "\n",
    "data_validation=np.array(data_validation)\n",
    "print(data_validation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expresso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'EVPDA3466_ENV_20240622_20240624173621.CSV' inserted into the database: 2.136241912841797 seconds\n"
     ]
    }
   ],
   "source": [
    "from ftplib import FTP\n",
    "import os\n",
    "import io\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "import time\n",
    "\n",
    "current_year = datetime.datetime.now().year\n",
    "\n",
    "ftp = FTP()\n",
    "ftp.connect(host='10.0.25.193', port=2122)\n",
    "ftp.login(user='cax_sgc', passwd='gy768#lo')\n",
    "\n",
    "ftp.cwd('/OP/Rotas/EventosPDA')\n",
    "\n",
    "files = ftp.nlst()\n",
    "\n",
    "def process_data(row):\n",
    "    try:\n",
    "        decoded_line = row.decode('Latin-1')\n",
    "    except UnicodeDecodeError as e:\n",
    "        print(f\"UnicodeDecodeError in file '{file}', line: {row}\")\n",
    "        decoded_line = row.decode('utf-8')\n",
    "    return decoded_line.strip()\n",
    "\n",
    "\n",
    "\n",
    "data_validation = []\n",
    "\n",
    "\n",
    "cur = con.cursor()\n",
    "\n",
    "cur.execute(\"SELECT distinct  SUBSTRING(DATA_CRIACAO, 7, 4) ||  SUBSTRING(DATA_CRIACAO, 4, 2) ||  SUBSTRING(DATA_CRIACAO, 1, 2) as dias FROM Expresso_2024 order by dias asc\")\n",
    "for record in cur.fetchall():\n",
    "    if record[0] not in data_validation:\n",
    "        data_validation.append(record[0])\n",
    "\n",
    "cur.close()\n",
    "\n",
    "for file in files:\n",
    "    start_time = time.time()\n",
    "    if file.startswith(\"E\"):\n",
    "        date_str_s = file[14:22].replace(\"/\", \"\")\n",
    "        ano = file[14:18]\n",
    "        date_str_s_array = np.array(date_str_s)\n",
    "        \n",
    "        if date_str_s not in data_validation and ano == \"2024\":\n",
    "            \n",
    "            ftp = FTP()\n",
    "            ftp.connect(host='10.0.25.193', port=2122)\n",
    "            ftp.login(user='cax_sgc', passwd='gy768#lo')\n",
    "            ftp.cwd('/OP/Rotas/EventosPDA')\n",
    "            \n",
    "            file_data = io.BytesIO()\n",
    "            ftp.retrbinary('RETR ' + file, file_data.write)\n",
    "            file_data.seek(0)  # Reset the stream position to the beginning\n",
    "\n",
    "            # Process the data line by line, excluding the last line\n",
    "            lines = file_data.readlines()\n",
    "            data_rows = [process_data(row) for row in lines[:-1]]  # Exclude the last line\n",
    "            df = pd.read_csv(io.StringIO('\\n'.join(data_rows)), sep=';', on_bad_lines='skip', encoding=\"Latin-1\")\n",
    "            ftp.quit()\n",
    "            \n",
    "            if len(data_rows)==1: \n",
    "                continue\n",
    "            else:\n",
    "                # Create a DataFrame from the cleaned data rows\n",
    "\n",
    "\n",
    "                # Insert the data into the database\n",
    "                for index, row in df.iterrows():\n",
    "                    try:\n",
    "                        cur = con.cursor()\n",
    "                        # Replace \"NaN\" values with None (NULL) for database insertion\n",
    "                        row = [None if pd.isna(value) else value for value in row]\n",
    "                        placeholders = ', '.join(['%s'] * len(row))\n",
    "                        insert_query = f\"INSERT INTO Expresso_2024 ({', '.join(df.columns)}) VALUES ({placeholders})\"\n",
    "                        cur.execute(insert_query, tuple(row))\n",
    "                    except (Exception, psycopg2.DatabaseError) as error:\n",
    "                        print(f\"Error inserting row {index+1}: {error}\")\n",
    "                        con.rollback()\n",
    "                    else:\n",
    "                        con.commit()\n",
    "                        \n",
    "                end_time = time.time()    \n",
    "                print(f\"File '{file}' inserted into the database: {end_time - start_time} seconds\")\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BANCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "def conecta_db():\n",
    "      con = psycopg2.connect(host='localhost', \n",
    "                         database='ctt2024',\n",
    "                         user='postgres', \n",
    "                         password='postgres')\n",
    "      return con\n",
    "\n",
    "\n",
    "def criar_db(sql):\n",
    "    con = conecta_db()\n",
    "    cur = con.cursor()\n",
    "    cur.execute(sql)\n",
    "    con.commit()\n",
    "    con.close()\n",
    "\n",
    "\n",
    "def inserir_db(sql):\n",
    "    con = conecta_db()\n",
    "    cur = con.cursor()\n",
    "    try:\n",
    "        cur.execute(sql)\n",
    "        con.commit()\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(\"Error: %s\" % error)\n",
    "        con.rollback()\n",
    "        cur.close()\n",
    "        return 1\n",
    "    cur.close()\n",
    "\n",
    "sql = ''' CREATE TABLE IF NOT EXISTS BANCA_2024 (\n",
    "                                    COD_CLIENTE_BNC text,\n",
    "                                    CODIGO_BALCAO_SIDIR text,\n",
    "                                    CODIGO_BALCAO_AF text,\n",
    "                                    AGENCIA text,\n",
    "                                    CA text,\n",
    "                                    CIRCUITO text,\n",
    "                                    DATA text,\n",
    "                                    CONTACTO text,\n",
    "                                    FATURAR text,\n",
    "                                    ENTREGUE text,\n",
    "                                    TAREFA text,\n",
    "                                    CP text,\n",
    "                                    CENTRO_OPERACIONAL text,\n",
    "                                    MORADA_TOQUE text,\n",
    "                                    HORA_TOQUE_PREVISTA text,\n",
    "                                    HORA_TOQUE_REAL text\n",
    "                                    ) '''\n",
    "criar_db(sql)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20240101' '20240102' '20240103' '20240104' '20240105' '20240108'\n",
      " '20240109' '20240110' '20240111' '20240112' '20240115' '20240116'\n",
      " '20240117' '20240118' '20240119' '20240122' '20240123' '20240124'\n",
      " '20240125' '20240126' '20240129' '20240130' '20240131' '20240201'\n",
      " '20240202' '20240205' '20240206' '20240207' '20240208' '20240209'\n",
      " '20240212' '20240213' '20240214' '20240215' '20240216' '20240219'\n",
      " '20240220' '20240221' '20240222' '20240223' '20240226' '20240227'\n",
      " '20240228' '20240229' '20240301' '20240304' '20240305' '20240306'\n",
      " '20240307' '20240308' '20240311' '20240312' '20240313' '20240314'\n",
      " '20240315' '20240318' '20240319' '20240320' '20240321' '20240322'\n",
      " '20240325' '20240326' '20240327' '20240328' '20240329' '20240401'\n",
      " '20240402' '20240403' '20240404' '20240405' '20240408' '20240409'\n",
      " '20240410' '20240411' '20240412' '20240415' '20240416' '20240417'\n",
      " '20240418' '20240419' '20240422' '20240423' '20240424' '20240425'\n",
      " '20240426' '20240429' '20240430' '20240501' '20240502' '20240503'\n",
      " '20240506' '20240507' '20240508' '20240509' '20240510' '20240513'\n",
      " '20240514' '20240515' '20240516' '20240517' '20240520' '20240521'\n",
      " '20240522' '20240523' '20240524' '20240527' '20240528' '20240529'\n",
      " '20240530' '20240531' '20240603' '20240604' '20240605' '20240606'\n",
      " '20240607' '20240610' '20240611' '20240612' '20240613' '20240614'\n",
      " '20240617' '20240618' '20240619' '20240620' '20240621' '20240624'\n",
      " '20240625' '20240626' '20240627' '20240628' '20240701' '20240702'\n",
      " '20240703' '20240704' '20240705' '20240708' '20240709' '20240710'\n",
      " '20240711' '20240712' '20240715' '20240716' '20240717' '20240718'\n",
      " '20240719' '20240722' '20240723' '20240724' '20240725' '20240726'\n",
      " '20240729' '20240730' '20240731' '20240801' '20240802' '20240805'\n",
      " '20240806' '20240807' '20240808' '20240809']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "data_validation = []\n",
    "con = conecta_db()\n",
    "cur = con.cursor(cursor_factory=psycopg2.extras.DictCursor)\n",
    "cur.execute(\"SELECT distinct REPLACE(data, '.', '') dias FROM Banca_2024 order by dias asc\")\n",
    "for record in cur.fetchall():\n",
    "    if record[0] not in data_validation:\n",
    "        data_validation.append(record[0])\n",
    "cur.close()\n",
    "\n",
    "data_validation=np.array(data_validation)\n",
    "print(data_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'CAX019_TOQUE_BANCA_D_20240812_20240813000504.CSV' inserted into the database: 0.8345110416412354 seconds\n",
      "File 'CAX019_TOQUE_BANCA_D_20240813_20240814000511.CSV' inserted into the database: 0.6749939918518066 seconds\n",
      "File 'CAX019_TOQUE_BANCA_D_20240814_20240815000505.CSV' inserted into the database: 0.7196526527404785 seconds\n",
      "File 'CAX019_TOQUE_BANCA_D_20240815_20240816000503.CSV' inserted into the database: 0.426929235458374 seconds\n",
      "File 'CAX019_TOQUE_BANCA_D_20240816_20240817000507.CSV' inserted into the database: 0.7912302017211914 seconds\n",
      "File 'CAX019_TOQUE_BANCA_D_20240819_20240820000506.CSV' inserted into the database: 0.805232048034668 seconds\n"
     ]
    }
   ],
   "source": [
    "from ftplib import FTP\n",
    "import os\n",
    "import io\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "import time\n",
    "\n",
    "current_year = datetime.datetime.now().year\n",
    "\n",
    "ftp = FTP()\n",
    "ftp.connect(host='10.0.25.193', port=2122)\n",
    "ftp.login(user='cax_sgc', passwd='gy768#lo')\n",
    "\n",
    "ftp.cwd('/OP/Rotas/Banca_Recolhas')\n",
    "\n",
    "files = ftp.nlst()\n",
    "\n",
    "def process_data(row):\n",
    "    try:\n",
    "        decoded_line = row.decode('Latin-1')\n",
    "    except UnicodeDecodeError as e:\n",
    "        print(f\"UnicodeDecodeError in file '{file}', line: {row}\")\n",
    "        decoded_line = row.decode('utf-8')\n",
    "    return decoded_line.strip()\n",
    "\n",
    "data_validation = []\n",
    "\n",
    "con = psycopg2.connect(host='localhost', \n",
    "                    database='ctt2024',\n",
    "                    user='postgres', \n",
    "                    password='postgres')\n",
    "\n",
    "cur = con.cursor()\n",
    "\n",
    "cur.execute(\"SELECT distinct REPLACE(data, '.', '') dias FROM Banca_2024 order by dias asc\")\n",
    "for record in cur.fetchall():\n",
    "    if record[0] not in data_validation:\n",
    "        data_validation.append(record[0])\n",
    "\n",
    "cur.close()\n",
    "\n",
    "# Create a mapping between input column names and database column names\n",
    "column_mapping = {\n",
    "    'ï»¿COD_CLIENTE_BNC': 'COD_CLIENTE_BNC',\n",
    "    'CODIGO_BALCAO_SIDIR': 'CODIGO_BALCAO_SIDIR',\n",
    "    'CODIGO_BALCAO_AF': 'CODIGO_BALCAO_AF',\n",
    "    'AGENCIA': 'AGENCIA',\n",
    "    'CA': 'CA',\n",
    "    'CIRCUITO': 'CIRCUITO',\n",
    "    'DATA': 'DATA',\n",
    "    'CONTACTO': 'CONTACTO',\n",
    "    'FATURAR': 'FATURAR',\n",
    "    'ENTREGUE': 'ENTREGUE',\n",
    "    'TAREFA': 'TAREFA',\n",
    "    'CP': 'CP',\n",
    "    'CENTRO OPERACIONAL': 'CENTRO_OPERACIONAL',\n",
    "    'MORADA TOQUE': 'MORADA_TOQUE',\n",
    "    'HORA TOQUE PREVISTA': 'HORA_TOQUE_PREVISTA',\n",
    "    'HORA TOQUE REAL': 'HORA_TOQUE_REAL'\n",
    "}\n",
    "\n",
    "for file in files:\n",
    "    start_time = time.time()\n",
    "    if file.startswith(\"C\"):\n",
    "        date_str_s = file[21:29].replace(\"/\", \"\")\n",
    "        date_str_s_array = np.array(date_str_s)\n",
    "        ano = file[21:25]\n",
    "        if date_str_s not in data_validation and ano ==\"2024\":\n",
    "            \n",
    "            ftp = FTP()\n",
    "            ftp.connect(host='10.0.25.193', port=2122)\n",
    "            ftp.login(user='cax_sgc', passwd='gy768#lo')\n",
    "            ftp.cwd('/OP/Rotas/Banca_Recolhas')\n",
    "            \n",
    "            file_data = io.BytesIO()\n",
    "            ftp.retrbinary('RETR ' + file, file_data.write)\n",
    "            file_data.seek(0)  # Reset the stream position to the beginning\n",
    "\n",
    "            # Process the data line by line, excluding the last line\n",
    "            lines = file_data.readlines()\n",
    "            data_rows = [process_data(row) for row in lines[:-1]]  # Exclude the last line\n",
    "            \n",
    "            # Remove BOM if present\n",
    "            if data_rows and data_rows[0].startswith('\\ufeff'):\n",
    "                data_rows[0] = data_rows[0][1:]\n",
    "            \n",
    "            # Join the data_rows without the last line (to exclude the empty line) and convert to bytes\n",
    "            csv_data = '\\n'.join(data_rows[:-1]).encode('utf-8')\n",
    "            \n",
    "            # Decode the bytes data using utf-8-sig to handle the BOM\n",
    "            decoded_data = csv_data.decode('utf-8-sig')\n",
    "            \n",
    "            if decoded_data.strip():  # Check if the decoded data is not empty\n",
    "                # Create a DataFrame from the cleaned data rows\n",
    "                df = pd.read_csv(io.StringIO(decoded_data), sep=';', on_bad_lines='skip', encoding=\"Latin-1\")\n",
    "\n",
    "                # Rename the DataFrame columns based on the mapping\n",
    "                df.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "                # Insert the data into the database\n",
    "                try:\n",
    "                    cur = con.cursor()\n",
    "                    # Replace \"NaN\" values with None (NULL) for database insertion\n",
    "                    df = df.where(pd.notna(df), None)\n",
    "\n",
    "                    # Generate placeholders for the query based on the number of columns\n",
    "                    placeholders = ', '.join(['%s'] * len(df.columns))\n",
    "\n",
    "                    # Construct the INSERT query with placeholders\n",
    "                    insert_query = f\"INSERT INTO Banca_2024 ({', '.join(df.columns)}) VALUES ({placeholders})\"\n",
    "\n",
    "                    # Execute the query with the DataFrame values as parameters\n",
    "                    psycopg2.extras.execute_batch(cur, insert_query, df.values)\n",
    "                    con.commit()\n",
    "\n",
    "                    end_time = time.time()\n",
    "                    print(f\"File '{file}' inserted into the database: {end_time - start_time} seconds\")\n",
    "                except (Exception, psycopg2.DatabaseError) as error:\n",
    "                    print(f\"Error inserting data from file '{file}': {error}\")\n",
    "                    con.rollback()\n",
    "\n",
    "ftp.quit()\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recolhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "def conecta_db():\n",
    "      con = psycopg2.connect(host='localhost', \n",
    "                         database='ctt2024',\n",
    "                         user='postgres', \n",
    "                         password='postgres')\n",
    "      return con\n",
    "\n",
    "\n",
    "def criar_db(sql):\n",
    "    con = conecta_db()\n",
    "    cur = con.cursor()\n",
    "    cur.execute(sql)\n",
    "    con.commit()\n",
    "    con.close()\n",
    "\n",
    "\n",
    "def inserir_db(sql):\n",
    "    con = conecta_db()\n",
    "    cur = con.cursor()\n",
    "    try:\n",
    "        cur.execute(sql)\n",
    "        con.commit()\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(\"Error: %s\" % error)\n",
    "        con.rollback()\n",
    "        cur.close()\n",
    "        return 1\n",
    "    cur.close()\n",
    "\n",
    "sql = '''CREATE TABLE IF NOT EXISTS Recolhas_2024 (\n",
    "            \"data_ficheiro\" text,\n",
    "            \"Código Orgânico\" text,\n",
    "            \"Descrição\" text,\n",
    "            \"Dia\" text,\n",
    "            \"Giro\" text,\n",
    "            \"Tipo\" text,\n",
    "            \"Ponto Recolha\" text,\n",
    "            \"Nome Cliente\" text,\n",
    "            \"Morada de recolha\" text,\n",
    "            \"Código postal recolha\" text,\n",
    "            \"Qntd de Objetos Recolhidos\" text,\n",
    "            \"Estado\" text,\n",
    "            \"Hora Início\" text,\n",
    "            \"Hora Fim\" text,\n",
    "            \"Hora Real\" text\n",
    "        )'''\n",
    "\n",
    "\n",
    "criar_db(sql)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20240101' '20240102' '20240103' '20240104' '20240105' '20240106'\n",
      " '20240107' '20240108' '20240109' '20240110' '20240111' '20240112'\n",
      " '20240113' '20240114' '20240115' '20240116' '20240117' '20240118'\n",
      " '20240119' '20240120' '20240121' '20240122' '20240123' '20240124'\n",
      " '20240125' '20240126' '20240127' '20240128' '20240129' '20240130'\n",
      " '20240131' '20240201' '20240202' '20240203' '20240204' '20240205'\n",
      " '20240206' '20240207' '20240208' '20240209' '20240210' '20240211'\n",
      " '20240212' '20240213' '20240214' '20240215' '20240216' '20240217'\n",
      " '20240218' '20240219' '20240220' '20240221' '20240222' '20240223'\n",
      " '20240224' '20240225' '20240226' '20240227' '20240228' '20240229'\n",
      " '20240301' '20240302' '20240303' '20240304' '20240305' '20240306'\n",
      " '20240307' '20240308' '20240309' '20240310' '20240311' '20240312'\n",
      " '20240313' '20240314' '20240315' '20240316' '20240317' '20240318'\n",
      " '20240319' '20240320' '20240321' '20240322' '20240323' '20240324'\n",
      " '20240325' '20240326' '20240327' '20240328' '20240329' '20240330'\n",
      " '20240331' '20240401' '20240402' '20240403' '20240404' '20240405'\n",
      " '20240406' '20240407' '20240408' '20240409' '20240410' '20240411'\n",
      " '20240412' '20240413' '20240414' '20240415' '20240416' '20240417'\n",
      " '20240418' '20240419' '20240420' '20240421' '20240422' '20240423'\n",
      " '20240424' '20240425' '20240426' '20240427' '20240428' '20240429'\n",
      " '20240430' '20240501' '20240502' '20240503' '20240504' '20240505'\n",
      " '20240506' '20240507' '20240508' '20240509' '20240510' '20240511'\n",
      " '20240512' '20240513' '20240514' '20240515' '20240516' '20240517'\n",
      " '20240518' '20240519' '20240520' '20240521' '20240522' '20240523'\n",
      " '20240524' '20240525' '20240526' '20240527' '20240528' '20240529'\n",
      " '20240530' '20240531' '20240601' '20240602' '20240603' '20240604'\n",
      " '20240605' '20240606' '20240607' '20240608' '20240609' '20240610'\n",
      " '20240611' '20240612' '20240613' '20240614' '20240615' '20240616'\n",
      " '20240617' '20240618' '20240619' '20240620' '20240621' '20240622'\n",
      " '20240623' '20240624' '20240625' '20240626' '20240627' '20240628'\n",
      " '20240629' '20240630' '20240701' '20240702' '20240703' '20240704'\n",
      " '20240705' '20240706' '20240707' '20240708' '20240709' '20240710'\n",
      " '20240711' '20240712' '20240713' '20240714' '20240715' '20240716'\n",
      " '20240717' '20240718' '20240719' '20240720' '20240721' '20240722'\n",
      " '20240723' '20240724' '20240725' '20240726' '20240727' '20240728'\n",
      " '20240729' '20240730' '20240731' '20240801' '20240802' '20240803'\n",
      " '20240804' '20240805' '20240806' '20240807' '20240808' '20240809'\n",
      " '20240810' '20240811']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "data_validation = []\n",
    "con = conecta_db()\n",
    "cur = con.cursor(cursor_factory=psycopg2.extras.DictCursor)\n",
    "\n",
    "cur.execute(\"SELECT DISTINCT  data_ficheiro as dias FROM Recolhas_2024  ORDER BY DIAS ASC\")\n",
    "for record in cur.fetchall():\n",
    "    if record[0] not in data_validation:\n",
    "        data_validation.append(record[0])\n",
    "cur.close()\n",
    "\n",
    "data_validation=np.array(data_validation)\n",
    "print(data_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'RECSG4366_RECOLHAS_20240812_20240813010501.CSV' inserted into the database: 1.4212870597839355 seconds\n",
      "File 'RECSG4366_RECOLHAS_20240813_20240814010500.CSV' inserted into the database: 2.059366226196289 seconds\n",
      "File 'RECSG4366_RECOLHAS_20240814_20240815010500.CSV' inserted into the database: 1.3753368854522705 seconds\n",
      "File 'RECSG4366_RECOLHAS_20240815_20240816010500.CSV' inserted into the database: 0.6690771579742432 seconds\n",
      "File 'RECSG4366_RECOLHAS_20240816_20240818001006.CSV' inserted into the database: 0.710547924041748 seconds\n",
      "File 'RECSG4366_RECOLHAS_20240817_20240818010500.CSV' inserted into the database: 0.6522409915924072 seconds\n",
      "File 'RECSG4366_RECOLHAS_20240818_20240819010500.CSV' inserted into the database: 0.7545249462127686 seconds\n",
      "File 'RECSG4366_RECOLHAS_20240819_20240820010500.CSV' inserted into the database: 1.433811902999878 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import time\n",
    "from ftplib import FTP\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "import datetime\n",
    "\n",
    "\n",
    "current_year = datetime.datetime.now().year\n",
    "\n",
    "ftp = FTP()\n",
    "ftp.connect(host='10.0.25.193', port=2122)\n",
    "ftp.login(user='cax_sgc', passwd='gy768#lo')\n",
    "\n",
    "ftp.cwd('/OP/Rotas/Banca_Recolhas')\n",
    "\n",
    "files = ftp.nlst()\n",
    "\n",
    "def process_data(row):\n",
    "    try:\n",
    "        decoded_line = row.decode('Latin-1')\n",
    "    except UnicodeDecodeError as e:\n",
    "        print(f\"UnicodeDecodeError in file '{file}', line: {row}\")\n",
    "        decoded_line = row.decode('utf-8')\n",
    "    return decoded_line.strip()\n",
    "\n",
    "data_validation = []\n",
    "\n",
    "con = psycopg2.connect(host='localhost', \n",
    "                    database='ctt2024',\n",
    "                    user='postgres', \n",
    "                    password='postgres')\n",
    "\n",
    "cur = con.cursor()\n",
    "\n",
    "cur.execute(\"SELECT DISTINCT  data_ficheiro as dias FROM Recolhas_2024  ORDER BY DIAS ASC\")\n",
    "\n",
    "for record in cur.fetchall():\n",
    "    if record[0] not in data_validation:\n",
    "        data_validation.append(record[0])\n",
    "\n",
    "# List of column names\n",
    "columns = ['\"Código Orgânico\"', '\"Descrição\"', '\"Dia\"', '\"Giro\"', '\"Tipo\"', '\"Ponto Recolha\"', '\"Nome Cliente\"',\n",
    "           '\"Morada de recolha\"', '\"Código postal recolha\"', '\"Qntd de Objetos Recolhidos\"', '\"Estado\"', '\"Hora Início\"',\n",
    "           '\"Hora Fim\"', '\"Hora Real\"']\n",
    "\n",
    "# Assuming you have a 'files' list defined earlier\n",
    "for file in files:\n",
    "    start_time = time.time()\n",
    "    if file.startswith(\"R\"):\n",
    "        date_str_s = file[19:27].replace(\"/\", \"\")\n",
    "        date_str_s_array = np.array(date_str_s)\n",
    "        ano = file[19:23]\n",
    "        if date_str_s not in data_validation and ano == \"2024\":\n",
    "\n",
    "            ftp = FTP()\n",
    "            ftp.connect(host='10.0.25.193', port=2122)\n",
    "            ftp.login(user='cax_sgc', passwd='gy768#lo')\n",
    "            ftp.cwd('/OP/Rotas/Banca_Recolhas')\n",
    "\n",
    "            file_data = io.BytesIO()\n",
    "            ftp.retrbinary('RETR ' + file, file_data.write)\n",
    "            file_data.seek(0)  # Reset the stream position to the beginning\n",
    "\n",
    "            # Process the data line by line, excluding the last line\n",
    "            lines = file_data.readlines()\n",
    "            data_rows = [process_data(row) for row in lines[:-1]]  # Exclude the last line\n",
    "\n",
    "            # Remove BOM if present\n",
    "            if data_rows and data_rows[0].startswith('\\ufeff'):\n",
    "                data_rows[0] = data_rows[0][1:]\n",
    "\n",
    "            # Join the data_rows without the last line (to exclude the empty line) and convert to bytes\n",
    "            csv_data = '\\n'.join(data_rows[:-1]).encode('utf-8')\n",
    "\n",
    "            # Decode the bytes data using utf-8-sig to handle the BOM\n",
    "            decoded_data = csv_data.decode('utf-8-sig')\n",
    "\n",
    "            if decoded_data.strip():  # Check if the decoded data is not empty\n",
    "                # Create a DataFrame from the cleaned data rows, specifying column names\n",
    "                df = pd.read_csv(io.StringIO(decoded_data), sep=';', names=columns, on_bad_lines='skip', encoding=\"Latin-1\")\n",
    "                df = df.iloc[1:]\n",
    "\n",
    "                # Add the 'data_ficheiro' column and fill it with date_str_s value\n",
    "                df['data_ficheiro'] = date_str_s\n",
    "\n",
    "                # Insert the data into the database\n",
    "                try:\n",
    "                    # Replace \"NaN\" values with None (NULL) for database insertion\n",
    "                    df = df.where(pd.notna(df), None)\n",
    "\n",
    "                    # Generate placeholders for the query based on the number of columns\n",
    "                    placeholders = ', '.join(['%s'] * len(df.columns))\n",
    "\n",
    "                    # Construct the INSERT query with placeholders\n",
    "                    insert_query = f\"INSERT INTO Recolhas_2024 ({', '.join(df.columns)}) VALUES ({placeholders})\"\n",
    "\n",
    "                    # Execute the query with the DataFrame values as parameters\n",
    "                    psycopg2.extras.execute_batch(cur, insert_query, df.values)\n",
    "                    con.commit()\n",
    "\n",
    "                    end_time = time.time()\n",
    "                    print(f\"File '{file}' inserted into the database: {end_time - start_time} seconds\")\n",
    "                except (Exception, psycopg2.DatabaseError) as error:\n",
    "                    print(f\"Error inserting data from file '{file}': {error}\")\n",
    "                    con.rollback()\n",
    "\n",
    "\n",
    "ftp.quit()\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REDE BASE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "def conecta_db():\n",
    "      con = psycopg2.connect(host='localhost', \n",
    "                         database='ctt2024',\n",
    "                         user='postgres', \n",
    "                         password='postgres')\n",
    "      return con\n",
    "\n",
    "\n",
    "def criar_db(sql):\n",
    "    con = conecta_db()\n",
    "    cur = con.cursor()\n",
    "    cur.execute(sql)\n",
    "    con.commit()\n",
    "    con.close()\n",
    "\n",
    "\n",
    "def inserir_db(sql):\n",
    "    con = conecta_db()\n",
    "    cur = con.cursor()\n",
    "    try:\n",
    "        cur.execute(sql)\n",
    "        con.commit()\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(\"Error: %s\" % error)\n",
    "        con.rollback()\n",
    "        cur.close()\n",
    "        return 1\n",
    "    cur.close()\n",
    "\n",
    "sql = ''' CREATE TABLE IF NOT EXISTS REDE_BASE_2024 (\n",
    "                                    DATA_CRIACAO      text,\n",
    "                                    CENTRO    text,\n",
    "                                    Giro text,\n",
    "                                    LOPTICA  text,\n",
    "                                    JANELA_HORARIA text,\n",
    "                                    NOME text,\n",
    "                                    MORADA text,\n",
    "                                    CP text,\n",
    "                                    LOCALIDADE text,\n",
    "                                    COD_T_EVEN text,\n",
    "                                    DATA_EVENTO text,\n",
    "                                    LATITUDE text,\n",
    "                                    LONGITUDE text,\n",
    "                                    NOME_REM text,\n",
    "                                    COD_PAIS_ORIGEM text) '''\n",
    "criar_db(sql)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20240102' '20240103' '20240104' '20240105' '20240106' '20240108'\n",
      " '20240109' '20240110' '20240111' '20240112' '20240113' '20240114'\n",
      " '20240115' '20240116' '20240117' '20240118' '20240119' '20240120'\n",
      " '20240122' '20240123' '20240124' '20240125' '20240126' '20240127'\n",
      " '20240128' '20240129' '20240130' '20240131' '20240201' '20240202'\n",
      " '20240203' '20240204' '20240205' '20240206' '20240207' '20240208'\n",
      " '20240209' '20240210' '20240211' '20240212' '20240213' '20240214'\n",
      " '20240215' '20240216' '20240217' '20240219' '20240220' '20240221'\n",
      " '20240222' '20240223' '20240224' '20240225' '20240226' '20240227'\n",
      " '20240228' '20240229' '20240301' '20240302' '20240304' '20240305'\n",
      " '20240306' '20240307' '20240308' '20240309' '20240311' '20240312'\n",
      " '20240313' '20240314' '20240315' '20240316' '20240318' '20240319'\n",
      " '20240320' '20240321' '20240322' '20240323' '20240325' '20240326'\n",
      " '20240327' '20240328' '20240329' '20240330' '20240401' '20240402'\n",
      " '20240403' '20240404' '20240405' '20240406' '20240408' '20240409'\n",
      " '20240410' '20240411' '20240412' '20240413' '20240415' '20240416'\n",
      " '20240417' '20240418' '20240419' '20240420' '20240422' '20240423'\n",
      " '20240424' '20240425' '20240426' '20240427' '20240429' '20240430'\n",
      " '20240501' '20240502' '20240503' '20240504' '20240505' '20240506'\n",
      " '20240507' '20240508' '20240509' '20240510' '20240511' '20240513'\n",
      " '20240514' '20240515' '20240516' '20240517' '20240518' '20240519'\n",
      " '20240520' '20240521' '20240522' '20240523' '20240524' '20240525'\n",
      " '20240527' '20240528' '20240529' '20240530' '20240531' '20240601'\n",
      " '20240603' '20240604' '20240605' '20240606' '20240607' '20240608'\n",
      " '20240610' '20240611' '20240612' '20240613' '20240614' '20240615'\n",
      " '20240617' '20240618' '20240619' '20240620' '20240621' '20240624'\n",
      " '20240625' '20240626' '20240627' '20240628' '20240629' '20240701'\n",
      " '20240702' '20240703' '20240704' '20240705' '20240706' '20240708'\n",
      " '20240709' '20240710' '20240711' '20240712' '20240713' '20240715'\n",
      " '20240716' '20240717' '20240718' '20240719' '20240720' '20240722'\n",
      " '20240723' '20240724' '20240725' '20240726' '20240727' '20240729'\n",
      " '20240730' '20240731' '20240801' '20240802' '20240803' '20240805'\n",
      " '20240806' '20240807' '20240808' '20240809' '20240810' 'NTCORO']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "data_validation = []\n",
    "con = conecta_db()\n",
    "cur = con.cursor(cursor_factory=psycopg2.extras.DictCursor)\n",
    "cur.execute(\"SELECT distinct  SUBSTRING(DATA_CRIACAO, 7, 4) ||  SUBSTRING(DATA_CRIACAO, 4, 2) ||  SUBSTRING(DATA_CRIACAO, 1, 2) as dias FROM REDE_BASE_2024 order by dias asc\")\n",
    "for record in cur.fetchall():\n",
    "    if record[0] not in data_validation:\n",
    "        data_validation.append(record[0])\n",
    "cur.close()\n",
    "\n",
    "data_validation=np.array(data_validation)\n",
    "print(data_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20240804\n",
      "File 'EVRBP4348_ENV_20240804_20240805010523.CSV' inserted into the database: 0.2836008071899414 seconds\n",
      "20240811\n",
      "File 'EVRBP4348_ENV_20240811_20240812010506.CSV' inserted into the database: 0.25863027572631836 seconds\n",
      "20240812\n",
      "File 'EVRBP4348_ENV_20240812_20240813010518.CSV' inserted into the database: 55.721471071243286 seconds\n",
      "20240813\n",
      "File 'EVRBP4348_ENV_20240813_20240814010501.CSV' inserted into the database: 55.51428985595703 seconds\n",
      "20240814\n",
      "File 'EVRBP4348_ENV_20240814_20240815010515.CSV' inserted into the database: 75.94307613372803 seconds\n",
      "20240815\n",
      "File 'EVRBP4348_ENV_20240815_20240816010524.CSV' inserted into the database: 1.0807209014892578 seconds\n",
      "20240816\n",
      "File 'EVRBP4348_ENV_20240816_20240817010513.CSV' inserted into the database: 62.69396996498108 seconds\n",
      "20240817\n",
      "File 'EVRBP4348_ENV_20240817_20240818010506.CSV' inserted into the database: 2.230713129043579 seconds\n",
      "20240818\n",
      "File 'EVRBP4348_ENV_20240818_20240819010501.CSV' inserted into the database: 0.27138495445251465 seconds\n",
      "20240819\n",
      "File 'EVRBP4348_ENV_20240819_20240820010521.CSV' inserted into the database: 94.89232516288757 seconds\n"
     ]
    }
   ],
   "source": [
    "from ftplib import FTP\n",
    "import os\n",
    "import io\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "import time\n",
    "\n",
    "current_year = datetime.datetime.now().year\n",
    "\n",
    "ftp = FTP()\n",
    "ftp.connect(host='10.0.25.193', port=2122)\n",
    "ftp.login(user='cax_sgc', passwd='gy768#lo')\n",
    "\n",
    "ftp.cwd('/OP/Rotas/EventosPDA_Rede_base')\n",
    "\n",
    "files = ftp.nlst()\n",
    "\n",
    "def process_data(row):\n",
    "    try:\n",
    "        decoded_line = row.decode('Latin-1')\n",
    "    except UnicodeDecodeError as e:\n",
    "        print(f\"UnicodeDecodeError in file '{file}', line: {row}\")\n",
    "        decoded_line = row.decode('utf-8')\n",
    "\n",
    "    # Check if the number of columns is 13 (assuming semicolon as the delimiter)\n",
    "    if len(decoded_line.split(';')) == 13:\n",
    "        return decoded_line.strip()\n",
    "    else:\n",
    "        print(f\"Ignoring row with incorrect number of columns: {decoded_line}\")\n",
    "        return None  # Skip the row if it has an incorrect number of columns\n",
    "\n",
    "data_validation = []\n",
    "\n",
    "cur = con.cursor()\n",
    "\n",
    "cur.execute(\"SELECT distinct  SUBSTRING(DATA_CRIACAO, 7, 4) ||  SUBSTRING(DATA_CRIACAO, 4, 2) ||  SUBSTRING(DATA_CRIACAO, 1, 2) as dias FROM REDE_BASE_2024 order by dias asc\")\n",
    "for record in cur.fetchall():\n",
    "    if record[0] not in data_validation:\n",
    "        data_validation.append(record[0])\n",
    "\n",
    "cur.close()\n",
    "\n",
    "for file in files:\n",
    "    start_time = time.time()\n",
    "    if file.startswith(\"E\"):\n",
    "        date_str_s = file[14:22].replace(\"/\", \"\")\n",
    "        month = file[18:20].replace(\"/\", \"\")\n",
    "        date_str_s = np.array(date_str_s)\n",
    "        ano = file[14:18]\n",
    "        if date_str_s not in data_validation and month >= \"08\" and ano == \"2024\":\n",
    "            print(date_str_s)\n",
    "\n",
    "            ftp = FTP()\n",
    "            ftp.connect(host='10.0.25.193', port=2122)\n",
    "            ftp.login(user='cax_sgc', passwd='gy768#lo')\n",
    "            ftp.cwd('/OP/Rotas/EventosPDA_Rede_base')\n",
    "\n",
    "            file_data = io.BytesIO()\n",
    "            ftp.retrbinary('RETR ' + file, file_data.write)\n",
    "            file_data.seek(0)  # Reset the stream position to the beginning\n",
    "\n",
    "            try:\n",
    "                df = pd.read_csv(file_data, sep=';', on_bad_lines='skip', encoding=\"Latin-1\")\n",
    "            except pd.errors.ParserError as pe:\n",
    "                print(f\"ParserError occurred while reading CSV in file '{file}':\")\n",
    "                print(pe)\n",
    "                print(f\"Skipping file '{file}'.\")\n",
    "                continue\n",
    "\n",
    "            ftp.quit()\n",
    "\n",
    "            if len(df) == 0:\n",
    "                print(f\"All rows in file '{file}' were problematic. Skipping insertion.\")\n",
    "                continue\n",
    "            else:\n",
    "                # Create a DataFrame from the cleaned data rows\n",
    "\n",
    "                # Insert the data into the database\n",
    "                for index, row in df.iterrows():\n",
    "                    try:\n",
    "                        cur = con.cursor()\n",
    "                        # Replace \"NaN\" values with None (NULL) for database insertion\n",
    "                        row = [None if pd.isna(value) else value for value in row]\n",
    "                        placeholders = ', '.join(['%s'] * len(row))\n",
    "                        insert_query = f\"INSERT INTO REDE_BASE_2024 ({', '.join(df.columns)}) VALUES ({placeholders})\"\n",
    "                        cur.execute(insert_query, tuple(row))\n",
    "                    except (Exception, psycopg2.DatabaseError) as error:\n",
    "                        print(f\"Error inserting row {index+1} from file '{file}': {error}\")\n",
    "                        con.rollback()\n",
    "                    else:\n",
    "                        con.commit()\n",
    "\n",
    "                end_time = time.time()\n",
    "                print(f\"File '{file}' inserted into the database: {end_time - start_time} seconds\")\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lista de tarefas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import psycopg2\n",
    "# import psycopg2.extras\n",
    "# import os\n",
    "\n",
    "\n",
    "\n",
    "# def conecta_db():\n",
    "#       con = psycopg2.connect(host='localhost', \n",
    "#                          database='ctt2024',\n",
    "#                          user='postgres', \n",
    "#                          password='postgres')\n",
    "#       return con\n",
    "\n",
    "\n",
    "# def criar_db(sql):\n",
    "#     con = conecta_db()\n",
    "#     cur = con.cursor()\n",
    "#     cur.execute(sql)\n",
    "#     con.commit()\n",
    "#     con.close()\n",
    "\n",
    "\n",
    "# def inserir_db(sql):\n",
    "#     con = conecta_db()\n",
    "#     cur = con.cursor()\n",
    "#     try:\n",
    "#         cur.execute(sql)\n",
    "#         con.commit()\n",
    "#     except (Exception, psycopg2.DatabaseError) as error:\n",
    "#         print(\"Error: %s\" % error)\n",
    "#         con.rollback()\n",
    "#         cur.close()\n",
    "#         return 1\n",
    "#     cur.close()\n",
    "\n",
    "# sql = ''' CREATE TABLE IF NOT EXISTS Lista_tarefas_2023 (\n",
    "#                                     LISTA_TAREFAS      text,\n",
    "#                                     GIRO    text,\n",
    "#                                     CENTRO text,\n",
    "#                                     COLABORADOR  text,\n",
    "#                                     CODIGO_SERVICO text,\n",
    "#                                     DATA_HORA_ENTRADA_LT text,\n",
    "#                                     TIPO_SERVICO text,\n",
    "#                                     QUANTIDADE text,\n",
    "#                                     NOME text,\n",
    "#                                     ESTADO_SERVICO text,\n",
    "#                                     DATA_SERVICO text,\n",
    "#                                     DATA_FECHO_LT text,\n",
    "#                                     EVENTO text,\n",
    "#                                     RAZAO_SITUACAO text) '''\n",
    "# criar_db(sql)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# data_validation = []\n",
    "# con = conecta_db()\n",
    "# cur = con.cursor(cursor_factory=psycopg2.extras.DictCursor)\n",
    "# cur.execute(\"SELECT distinct  SUBSTRING(LISTA_TAREFAS, 1, 8) as dias FROM Lista_tarefas_2023 order by dias asc\")\n",
    "# for record in cur.fetchall():\n",
    "#     if record[0] not in data_validation:\n",
    "#         data_validation.append(record[0])\n",
    "# cur.close()\n",
    "\n",
    "# data_validation=np.array(data_validation)\n",
    "# print(data_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ftplib import FTP\n",
    "# import os\n",
    "# import io\n",
    "# import datetime\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import psycopg2\n",
    "# import psycopg2.extras\n",
    "# import time\n",
    "\n",
    "# current_year = datetime.datetime.now().year\n",
    "\n",
    "# ftp = FTP()\n",
    "# ftp.connect(host='10.0.25.193', port=2122)\n",
    "# ftp.login(user='cax_sgc', passwd='gy768#lo')\n",
    "\n",
    "# ftp.cwd('/OP/Rotas/Listas_Tarefas')\n",
    "\n",
    "# files = ftp.nlst()\n",
    "\n",
    "\n",
    "# def process_data(row):\n",
    "#     try:\n",
    "#         decoded_line = row.decode('Latin-1')\n",
    "#     except UnicodeDecodeError as e:\n",
    "#         print(f\"UnicodeDecodeError in file '{file}', line: {row}\")\n",
    "#         decoded_line = row.decode('utf-8')\n",
    "#     return decoded_line.strip()\n",
    "\n",
    "# data_validation = []\n",
    "\n",
    "\n",
    "# cur = con.cursor()\n",
    "\n",
    "# cur.execute(\"SELECT distinct  SUBSTRING(LISTA_TAREFAS, 1, 8) as dias FROM Lista_tarefas_2023 order by dias asc\")\n",
    "# for record in cur.fetchall():\n",
    "#     if record[0] not in data_validation:\n",
    "#         data_validation.append(record[0])\n",
    "\n",
    "# cur.close()\n",
    "\n",
    "# for file in files:\n",
    "    \n",
    "#     start_time = time.time()\n",
    "#     if file.startswith(\"L\"):\n",
    "#         Date_str_s = file[11:19].replace(\"/\", \"\")\n",
    "#         month= file[15:17].replace(\"/\", \"\")\n",
    "#         date_str_s= np.array(Date_str_s)\n",
    "#         if date_str_s not in data_validation and month == \"12\":        \n",
    "#             print(date_str_s)\n",
    "#             ftp = FTP()\n",
    "#             ftp.connect(host='10.0.25.193', port=2122)\n",
    "#             ftp.login(user='cax_sgc', passwd='gy768#lo')\n",
    "#             ftp.cwd('/OP/Rotas/Listas_Tarefas')\n",
    "            \n",
    "#             file_data = io.BytesIO()\n",
    "#             ftp.retrbinary('RETR ' + file, file_data.write)\n",
    "#             file_data.seek(0)  # Reset the stream position to the beginning\n",
    "\n",
    "#             # Process the data line by line, excluding the last line\n",
    "#             lines = file_data.readlines()\n",
    "#             data_rows = [process_data(row) for row in lines[:-1]]  # Exclude the last line\n",
    "            \n",
    "#             try:\n",
    "#                 df = pd.read_csv(io.StringIO('\\n'.join(data_rows)), sep=';')\n",
    "#             except pd.errors.ParserError as pe:\n",
    "#                 print(f\"ParserError occurred while reading CSV in file '{file}':\")\n",
    "#                 print(pe)\n",
    "#                 continue  \n",
    "#             #df = pd.read_csv(io.StringIO('\\n'.join(data_rows)), sep=';')\n",
    "            \n",
    "#             ftp.quit()\n",
    "            \n",
    "#             if len(data_rows)==1: \n",
    "#                 continue\n",
    "#             else:\n",
    "#                 # Create a DataFrame from the cleaned data rows\n",
    "\n",
    "\n",
    "#                 # Insert the data into the database\n",
    "#                 for index, row in df.iterrows():\n",
    "#                     try:\n",
    "#                         cur = con.cursor()\n",
    "#                         # Replace \"NaN\" values with None (NULL) for database insertion\n",
    "#                         row = [None if pd.isna(value) else value for value in row]\n",
    "#                         placeholders = ', '.join(['%s'] * len(row))\n",
    "#                         insert_query = f\"INSERT INTO Lista_tarefas_2023 ({', '.join(df.columns)}) VALUES ({placeholders})\"\n",
    "#                         cur.execute(insert_query, tuple(row))\n",
    "#                     except (Exception, psycopg2.DatabaseError) as error:\n",
    "#                         print(f\"Error inserting row {index+1}: {error}\")\n",
    "#                         con.rollback()\n",
    "#                     else:\n",
    "#                         con.commit()\n",
    "                        \n",
    "#                 end_time = time.time()    \n",
    "#                 print(f\"File '{file}' inserted into the database: {end_time - start_time} seconds\")\n",
    "\n",
    "\n",
    "# con.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
